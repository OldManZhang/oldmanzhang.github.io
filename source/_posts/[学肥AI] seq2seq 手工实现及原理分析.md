---
title: seq2seq æ‰‹å·¥å®ç°åŠåŸç†åˆ†æ
date: 2024-08-13 10:18:38
tags: 
    - å­¦è‚¥AI
    - seq2seq
    - æ·±åº¦å­¦ä¹ 
categories: å­¦è‚¥AI
description: "ç°å®ä¸­ï¼Œæœ‰ä¸€ç±»é—®é¢˜æ˜¯ è¾“å…¥è¾“å‡ºä¸å®šé•¿ çš„ï¼Œæ¯”å¦‚ 
1. ç¿»è¯‘ï¼Œä»ä¸­æ–‡åˆ°è‹±æ–‡
2. æ–‡ç”Ÿå›¾ï¼Œä¸€æ®µè¯ç”Ÿæˆä¸€ä¸ªå›¾ç‰‡
3. æ‘˜è¦ï¼Œæ€»ç»“ä¸€æ®µè¯çš„ä¿¡æ¯
æ‰€ä»¥ seq2seq å°±æ˜¯ä¸ºäº†è§£å†³è¿™ç§ ä¸€ä¸²åºåˆ— ç”Ÿæˆ å¦å¤–ä¸€ä¸²åºåˆ— é—®é¢˜çš„æ¨¡å‹ã€‚"
cover: https://qiniu.oldzhangtech.com/cover/%E5%A4%A7%E8%B0%B7%E7%BF%94%E5%B9%B3%202.jpg
---


## èƒŒæ™¯é—®é¢˜
ç°å®ä¸­ï¼Œæœ‰ä¸€ç±»é—®é¢˜æ˜¯ **è¾“å…¥è¾“å‡ºä¸å®šé•¿** çš„ï¼Œæ¯”å¦‚

1. ç¿»è¯‘ï¼Œä»ä¸­æ–‡åˆ°è‹±æ–‡
2. æ–‡ç”Ÿå›¾ï¼Œä¸€æ®µè¯ç”Ÿæˆä¸€ä¸ªå›¾ç‰‡
3. æ‘˜è¦ï¼Œæ€»ç»“ä¸€æ®µè¯çš„ä¿¡æ¯

æ‰€ä»¥ `seq2seq` å°±æ˜¯ä¸ºäº†è§£å†³è¿™ç§ ä¸€ä¸²åºåˆ— ç”Ÿæˆ å¦å¤–ä¸€ä¸²åºåˆ— é—®é¢˜çš„æ¨¡å‹ã€‚
## åŸç†
`seq2seq`ï¼Œ`sequence to sequence`ï¼Œä¹Ÿæœ‰å¦å¤–ä¸€ç§å«æ³• `encoder and decoder`ã€‚ä»–æ˜¯ä¸€ç§ä¸Šå±‚æ¨¡å‹æ¶æ„ï¼Œå³æ˜¯ç»„åˆæ¨¡å‹ï¼Œä»–å¯ä»¥ç”±ä¸åŒçš„åº•å±‚æ¨¡å‹æ¥å®ç°ã€‚

æˆ‘ä»¬å¯ä»¥å…ˆçœ‹åŸç†å›¾ã€‚
_åŸç†å›¾_  
![](https://qiniu.oldzhangtech.com/mdpic/65e94f22-668b-4d3f-accc-57617096e7d4_cd0a100f-9d6f-44db-86c6-5061e8e974ae.jpeg)
ä»åŸç†å›¾ä¸­å¯ä»¥çŸ¥é“ï¼Œ`seq2seq `æ¨¡å‹ æœ‰ä»¥ä¸‹çš„ç‰¹å¾ï¼š

1. æ¨¡å‹éƒ½ä¼šæœ‰ä¸€ä¸ª `Encoder` ï¼Œä¸€ä¸ª `Decoder`ï¼Œå’Œä¸€ä¸ª `Context`
2. `Encoder` å°±æ˜¯å­—é¢æ„æ€çš„ -- ç¼–ç å™¨ï¼Œ`src_input` ç»è¿‡`Encoder` å¤„ç†ï¼Œè¾“å‡º `Context` ä¸­
3. åŒç†ï¼Œ`Decoder` å°±æ˜¯è§£ç å™¨ï¼Œ`tgt_input` å’Œ `Context` ç»è¿‡ `Decoder` å¤„ç†, è¾“å‡º `tgt_output`
4. `Encoder` å’Œ `Decoder` éƒ½å¿…é¡»èƒ½å¤Ÿè¯†åˆ« `Context`
> srcï¼š sourceï¼Œ tgtï¼š target


ğŸ”¥ `Context` çš„ç»„æˆæ˜¯éå¸¸é‡è¦çš„ï¼Œä»–æ˜¯ `Encoder` å’Œ `Decoder` æ˜¯èƒ½å¤Ÿè¯†åˆ«çš„ä¸€ä¸ªä»‹è´¨ï¼Œæ˜¯é“¾æ¥ä¸¤è€…çš„æ¡¥æ¢ã€‚è¿™ç§ä»‹è´¨å¯ä»¥æ˜¯ _éšçŠ¶æ€_ï¼Œå¯ä»¥æ˜¯ _æ³¨æ„åŠ›çš„åŠ æƒè®¡ç®—å€¼_ï¼Œç­‰ç­‰ï¼Œè¿™äº›éƒ½ç”±åº•å±‚çš„æ¨¡å‹æ¥å†³å®šçš„ã€‚

å°±å¥½æ¯”å›½é™…è´¸æ˜“ä¸­ï¼Œæˆ‘ä»¬æƒ³ä¹°æ¾³å¤§åˆ©äºšé“çŸ¿ã€‚ ç¾å…ƒæ˜¯ç¡¬é€šè´§ï¼Œä¸­é—´ä»‹è´¨ï¼ŒZG å’Œ åœŸæ¾³ éƒ½è®¤ç¾å…ƒï¼Œæ‰€ä»¥ ZG encoder å…ˆæŠŠ RMB è½¬æˆ Dollarï¼Œç»™åˆ°åœŸæ¾³ decoderï¼ŒåœŸæ¾³å†æ¢å›è‡ªå·±çš„ æ¾³å…ƒã€‚

ğŸ”¥ **ä¸å®šé•¿**ï¼Œè¾“å…¥å€¼ï¼ˆæ¯”å¦‚ï¼Œé•¿åº¦æ˜¯ 8ï¼‰åœ¨ `Encoder` éƒ½è½¬æ¢æˆç»Ÿä¸€çš„ `Context`ï¼ˆæ¯”å¦‚ï¼Œ128 X 512 çš„ 2 å±‚ç¥ç»ç½‘ç»œï¼‰ï¼ŒåŒæ—¶ è¾“å‡ºå€¼çš„é•¿åº¦ï¼ˆæ¯”å¦‚ï¼Œé•¿åº¦æ˜¯ 10 ï¼‰ ç”± `Decoder` å’Œ `Context`  æ¥å†³å®šï¼Œå·²ç»ä¸è¾“å…¥å€¼æ— å…³äº†ã€‚

åŒæ—¶ï¼Œ`seq2seq` ä»…ä»…æ˜¯ä¸Šå±‚æ¶æ„ï¼Œåº•å±‚å®ç°çš„æ¨¡å‹æ˜¯å•¥éƒ½å¯ä»¥è§†æƒ…å†µè€Œå®šã€‚æ¯”å¦‚ï¼Œåº•å±‚å¯ä»¥æ˜¯ `RNN`ï¼Œå¯ä»¥æ˜¯ `LSTM`ï¼Œä¹Ÿå¯ä»¥æ˜¯ `GRU`ï¼Œ ä¹Ÿå¯ä»¥æ˜¯ `Transformer`ã€‚æœ¬æ–‡ä¾‹å­ä¸­ä½¿ç”¨ `RNN` æ¥å®ç°ã€‚
## ä¾‹å­ -- ç¿»è¯‘
> ä¸‹é¢æ˜¯æ‰‹å·¥å®ç°ä¸€ä¸ªåŸºäº `RNN` çš„ `seq2seq` æ¨¡å‹ã€‚å¯è¿è¡Œçš„ ipynb æ–‡ä»¶çš„[é“¾æ¥](https://gitee.com/oldmanzhang/resource_machine_learning/blob/master/deep_learning/seq2seq.ipynb)ã€‚

### ä»»åŠ¡ç›®æ ‡   
ä¾‹å­çš„ç›®æ ‡ï¼Œä»æœ‰é™çš„ç¿»è¯‘èµ„æ–™ä¸­ï¼Œè®­ç»ƒå‡ºç¿»è¯‘çš„é€»è¾‘ï¼Œå®ç°ä»è‹±æ–‡ç¿»è¯‘æˆæ³•æ–‡ã€‚

### åˆ†æä»»åŠ¡  
> è¿™é‡Œå…ˆä¸è®¨è®ºå­—ç¬¦çš„å¤„ç†æµç¨‹ï¼ˆæ¸…æ´—å­—ç¬¦ï¼Œè¿‡æ»¤ç‰¹æ®Šå­—ç¬¦ç­‰ï¼‰ï¼Œæ‰€æœ‰çš„æµç¨‹ç®€å•åŒ–ï¼Œä»…ä»…æ˜¯éªŒè¯æ¨¡å‹çš„ä½¿ç”¨ã€‚

1. ç¿»è¯‘æ˜¯ä¸€ä¸ªâ€œåˆ†ç±»â€ä»»åŠ¡
2. è¿™ä¸ªæ˜¯ä¸€ä¸ªä¸å®šé•¿çš„è¾“å…¥å’Œè¾“å‡ºçš„ï¼Œæ‰€ä»¥ä½¿ç”¨ `seq2seq` çš„æ¨¡å‹
3. åŒæ—¶è¾“å…¥å’Œè¾“å‡ºæ˜¯æœ‰æ—¶é—´åºåˆ—çš„ï¼Œæ‰€ä»¥åº•å±‚æ¨¡å‹ä½¿ç”¨å¸¦æœ‰è®°å¿†èƒ½åŠ›çš„æ¨¡å‹ï¼Œæˆ‘ä»¬ä½¿ç”¨ `RNN`

â“ ä¸ºä»€ä¹ˆæ˜¯ä¸€ä»½åˆ†ç±»çš„ä»»åŠ¡ï¼Ÿ
è¿™å…¶å®æ˜¯ `word2index` çš„è¿‡ç¨‹ï¼Œæ¯ä¸ª `word` å°±æ˜¯ä¸€ä¸ªåˆ†ç±»ã€‚ä¸¾ä¾‹ï¼šæ¯”å¦‚ è¾“å…¥çš„æ˜¯è‹±æ–‡ï¼Œè‹±æ–‡ä¸­çš„ä¸€å…±æœ‰ 4000 ä¸ªå•è¯ï¼Œé‚£ä¹ˆè¾“å…¥çš„åˆ†ç±»å°±æ˜¯ 4000 ï¼›è¾“å‡ºçš„æ˜¯æ³•æ–‡ï¼Œæ³•æ–‡ä¸­çš„ä¸€å…±æœ‰ 2000 ä¸ªå•è¯ï¼Œé‚£ä¹ˆè¾“å‡ºçš„åˆ†ç±»å°±æ˜¯ 2000ã€‚

### ä»£ç ç»“æ„
![](https://qiniu.oldzhangtech.com/mdpic/efa06cc1-1371-4e15-a0fd-0e5505739279_54099b97-f43c-47fc-ab0a-7f855bc6ad50.jpeg)
ä¸Šå›¾æ˜¯ æ•°æ®åœ¨ seq2seq æµåŠ¨ä¸­ä¸²èµ·ä¸åŒç»„ä»¶çš„è¿‡ç¨‹ã€‚


_ç»„ä»¶è¯´æ˜ï¼š_   

1. `word_index`ï¼Œå°±æ˜¯æŠŠå•è¯è½¬æ¢æˆ `index`
2. `embedding`ï¼Œå°±è¦æŠŠç¦»æ•£çš„ `index` è½¬æ¢æˆå¯ä»¥è®¡ç®—çš„è¿ç»­çš„ `embedding`ï¼Œé€‚åˆæ¨¡å‹çš„è®¡ç®—
3. `word_index` å’Œ `embedding` æ­£å¸¸æƒ…å†µæ˜¯ è¾“å…¥å’Œè¾“å‡ºéƒ½ä¸èƒ½å…±ç”¨çš„
4. `encoder` é‡Œé¢æœ‰ `embedding`ï¼Œ`rnn`
   1. `rnn` è¾“å…¥ `src`ï¼Œ è¾“å‡º `hidden` éšçŠ¶æ€ï¼Œå³ `Context`
5. `decoder` é‡Œé¢æœ‰ `embedding`ï¼Œ`rnn`ï¼Œ`full_connect`
   1. `rnn `**å¾ªç¯**å åŠ è¾“å…¥ `tgt_input` å’Œ `Context`ï¼Œ è¾“å‡º `new hidden`,  `tgt_output`
   2. `full_connect` è´Ÿè´£æŠŠ `tgt_output` ç”ŸæˆçœŸæ­£çš„ `real_tgt_ouput`
> äº†è§£ä»–ä»¬çš„å…·ä½“èŒè´£åå†å»çœ‹ä»–ä»¬çš„ä»£ç å°±æ¸…æ™°å¤šäº†


_ä»£ç ç‰‡æ®µåˆ†æ_  
```python
# Define the Encoder RNN
class EncoderRNN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(EncoderRNN, self).__init__()
        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(input_size, hidden_size)
        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)
    
    def forward(self, input_seq, hidden):
        # å†…éƒ¨è¿›è¡Œ embedding
        # ä¼ å…¥çš„æ˜¯ input_indices
        embedded = self.embedding(input_seq)
        output, hidden = self.rnn(embedded, hidden)
        return output, hidden
    
    def init_hidden(self):
        return torch.zeros(1, 1, self.hidden_size)
```
ä¸Šé¢æ˜¯ `encoder` çš„ä»£ç ï¼Œä½œç”¨å°±æ˜¯ï¼š

1. `src_input` è½¬æˆ `embedding`
2. `rnn` æŠŠ `embedding` è½¬æˆ `hidden`ï¼Œå³ `Context`


```python
# Define the Decoder RNN
class DecoderRNN(nn.Module):
    def __init__(self, hidden_size, output_size):
        super(DecoderRNN, ).__init__()
        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(output_size, hidden_size)
        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)
        self.out = nn.Linear(hidden_size, output_size)
    
    def forward(self, input_seq, hidden):
        # å†…éƒ¨è¿›è¡Œ embedding
        # ä¼ å…¥çš„æ˜¯ input_indices
        embedded = self.embedding(input_seq)
        output, hidden = self.rnn(embedded, hidden)
        # å°±æ˜¯ å…¨é“¾æ¥å±‚ ä» hidden -ã€‹ output_feature
        output = self.out(output.squeeze(1))
        return output, hidden

    def init_hidden(self):
        return torch.zeros(1, 1, self.hidden_size)
```
ä¸Šé¢æ˜¯ `dncoder` çš„ä»£ç ï¼Œä¸ `encoder `æ¯”è¾ƒå¤šäº†ä¸€ä¸ª `full connect` ä½¿ç”¨

1. `tgt_input` è½¬æˆ `embedding`
2. `rnn` æŠŠ `embedding` è½¬æˆ `hidden` å’Œ `output`
3. `full conect `å†æŠŠ `output` è½¬æˆ `output_feature`

```python
# Define the Seq2Seq model
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder):
        super(Seq2Seq, self).__init__()
        # [1]
        self.encoder = encoder
        self.decoder = decoder
    
    def forward(self, src_seq, tgt_seq, teacher_forcing_ratio=0.5):
        batch_size = src_seq.size(0)
        # count of words [[0, 1, 2, 9]]
        max_len = tgt_seq.size(1)
        # 11
        tgt_vocab_size = self.decoder.out.out_features
        # æœ‰ 11 ä¸ªéšçŠ¶æ€ï¼Œå°±æ˜¯ target ä¸­çš„å”¯ä¸€å€¼
        outputs = torch.zeros(batch_size, max_len, tgt_vocab_size)
        
        encoder_hidden = self.encoder.init_hidden()
        # encoder çš„ä½œç”¨æ˜¯ è¾“å‡º hiddenï¼Œ output å°±æ²¡æœ‰ä»€ä¹ˆæ„ä¹‰äº†
        # [2]
        encoder_output, encoder_hidden = self.encoder(src_seq, encoder_hidden)
        
        # tgt_seq ä½œç”¨ï¼Œå°±æ˜¯å–å¾—ç¬¬ä¸€ä¸ª <sos> token
        decoder_input = tgt_seq[:, 0].unsqueeze(1)  # Start with <sos>
        decoder_hidden = encoder_hidden
        
        # tgt_seq ä½œç”¨ï¼Œæˆªå–è¾“å‡ºçš„é•¿åº¦
        # ä¸å– 0ï¼Œæ˜¯å› ä¸º â€œ0â€œ index æ˜¯ä¸€ä¸ª <sos>
        # [3]
        for t in range(1, max_len):
            # [4]
            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)
            # decoder_output shape (1,11)ï¼Œå…¶å®æ˜¯ä¸€ä¸ªå¤šåˆ†ç±»çš„é—®é¢˜
            # ä¸ outputs[:, t] = decoder_output æ˜¯ä¸€æ ·çš„ï¼Œå› ä¸º batch_size æ’ç­‰äº 1ï¼Œæ‰€ä»¥æš‚æ—¶å½±å“ä¸å¤§ï¼Œä½†æ˜¯å®é™…åº”ç”¨ä¸­ï¼Œåº”è¯¥è¦æ”¹æˆå¯¹åº”çš„ batch
            outputs[:, t, :] = decoder_output
            top1 = decoder_output.argmax(1).unsqueeze(1)
            # è¿™é‡Œæ˜¯å–å·§äº†ï¼Œteacher_forcing_ratio æ˜¯å–å·§äº†ã€‚
            # decode_input_t+1 æœ‰æ—¶æ˜¯ decode_output_tï¼Œ æœ‰æ—¶æ˜¯ real_target_seq_t
            # [5]
            decoder_input = tgt_seq[:, t].unsqueeze(1) if random.random() < teacher_forcing_ratio else top1
        
        return outputs
```
ä¸Šé¢çš„ä»£ç æ˜¯ `seq2seq` æ¨¡å‹çš„å®šä¹‰ã€‚

_è®­ç»ƒè¿‡ç¨‹_   
å¯ä»¥æ£€æŸ¥æ•°æ®åœ¨è¿™ä¸ªæ¨¡å‹ä¸­æµåŠ¨å¦‚ä¸‹ï¼š

1. [1] é‡Œé¢åŒ…å«äº†ä¸€ä¸ª `encoder` å’Œ `decoder`
2. [2] `forword` æ—¶,  `encoder` è½¬æ¢ `src_input` æˆ `hidden`
3. [3] å¼€å§‹ `decoder` å¾ªç¯ï¼Œæœ€å¤§é•¿åº¦æ˜¯ `max_len`ã€‚åˆå§‹åŒ–å³æ˜¯ï¼š `decoder_input = â€œ<sos> indexâ€œ`ï¼Œ`decoder_hidden = encoder_hidden`
4. [4] `decoder` è¾“å‡ºæ˜¯ `output_index` + `new_hidden`
5. [5] `decoder_input+= output_index`, `decoder_hidden += new_hidden` å åŠ åå†èµ°æ­¥éª¤ [3] å¾ªç¯

ğŸ’¡ `teacher_forcing` æ˜¯ä»€ä¹ˆï¼Ÿ
å°±æ˜¯è®­ç»ƒçš„æ—¶å€™ï¼Œæœ‰ä¸€å®šçš„æ¦‚ç‡è¾“å‡ºæ˜¯ _çœŸå®å€¼_ è€Œä¸æ˜¯ _é¢„æµ‹å€¼_ã€‚å°±èƒ½æ˜¯æ¨¡å‹æ›´åŠ å¿«çš„æ”¶æ•›ï¼ŒåŠ é€Ÿæ¨¡å‹çš„å­¦ä¹ ã€‚ä½†æ˜¯è¿‡äºä¾èµ– _çœŸå®å€¼_ï¼Œå°±ä¼šå¯¼è‡´æ³›åŒ–èƒ½åŠ›å·®ã€‚`teacher_forcing_ratio` å°±å¯ä»¥è°ƒæ•´é˜ˆå€¼ã€‚

_æ¨ç†è¿‡ç¨‹_  
```python
        input_seq = torch.tensor(indices, dtype=torch.long).unsqueeze(0)  # (1, seq_len)
        # å…¨éƒ¨çš„inputï¼Œéƒ½è½¬æˆ hidden
        encoder_hidden = model.encoder.init_hidden()
        # encoder å’Œ decoder çš„ä½¿ç”¨æ˜¯åˆ†å¼€çš„ã€
        # [1]
        encoder_output, encoder_hidden = model.encoder(input_seq, encoder_hidden)
        # [2]
        decoder_input = torch.tensor([[fra_word2idx['<sos>']]], dtype=torch.long)  # Start token
        decoder_hidden = encoder_hidden
        
        translated_sentence = []

        # [3]
        for _ in range(max_length):
            # decoder_input æ˜¯é€æ­¥çš„ç´¯åŠ çš„ï¼Œå°±æ˜¯ word1+word2+word3...
            # ç¬¬ä¸€ä¸ª decoder_hidden æ˜¯ encoder_hidden
            # ä»ç¬¬äºŒä¸ªå¼€å§‹ï¼Œå°±æ˜¯å¾ªç¯å¾—åˆ° decoder_hidden ä¸åœçš„ä¼ å…¥
            # encoder å’Œ decoder çš„ä½¿ç”¨æ˜¯åˆ†å¼€çš„
            # [4]
            decoder_output, decoder_hidden = model.decoder(decoder_input, decoder_hidden)
            top1 = decoder_output.argmax(1).item()
            # "<UNK>", which stands for â€œunknown.â€
            # [5]
            translated_word = fra_idx2word.get(top1, "<UNK>")
            translated_sentence.append(translated_word)

            # [6]
            if translated_word == '<eos>':  # End of sentence
                break
            
            decoder_input = torch.tensor([[top1]], dtype=torch.long)  # Next input token
        
        return translated_sentence
```
_æ¨ç†è¿‡ç¨‹_ å’Œ _è®­ç»ƒè¿‡ç¨‹_ï¼Œå…·ä½“åŸç†ä¸€è‡´ã€‚ æœ‰ä»¥ä¸‹çš„å·®å¼‚ç‚¹éœ€è¦æ³¨æ„ï¼š

1. å¦‚ä½•å®šä¹‰å¼€å§‹è¾“å‡ºçš„æ ‡å¿—
2. å¦‚ä½•å®šä¹‰ç»“æŸè¾“å‡ºçš„æ ‡å¿—
3. å¦‚ä½•å®šä¹‰ä¸è®¤è¯†å­—ç¬¦çš„æ ‡å¿—

ä»£ç åˆ†æï¼š

1. [1] å•ç‹¬ä½¿ç”¨ `seq2seq's encoder`ï¼Œä¸” _ä¸€æ¬¡æ€§_ ç”Ÿæˆ `encoder hidden`
2. [2] `decoder_input` åˆå§‹åŒ–ï¼Œä»¥  '<sos>' å¼€å¤´ï¼Œæ ‡å¿—å¼€å§‹è¾“å‡º
3. [3] `decoder` å¼€å§‹å¾ªç¯
   1. [4] å•ç‹¬ä½¿ç”¨ `seq2seq's decoder`, è¾“å‡º `ouput `å’Œ `new_hidden`
   2. [5] ç¢°åˆ°ä¸è®¤è¯†çš„åˆ†ç±»ï¼Œå°±ä½¿ç”¨ '<UNK>'å–ä»£
   3. [6] å¦‚æœé‡åˆ° '<eos>' å­—ç¬¦å°±ç›´æ¥ç»“æŸå¾ªç¯
   4. å›åˆ° [3] ç»§ç»­å¾ªç¯

### ç»“æœ
```python
# è®­ç»ƒç»“æœ
Epoch: 0, Loss: 2.820215034484863
Epoch: 100, Loss: 1.0663029670715332
Epoch: 200, Loss: 1.1840879678726197
Epoch: 300, Loss: 1.224123215675354
Epoch: 400, Loss: 1.0645174384117126
Epoch: 500, Loss: 1.061875820159912
Epoch: 600, Loss: 1.0744179487228394
Epoch: 700, Loss: 1.0767890691757203
Epoch: 800, Loss: 1.099305510520935
Epoch: 900, Loss: 1.1019723176956178


# é¢„æµ‹
test_sentence = ["i", "am"]
translation = translate(model, test_sentence)
print("Translation:", " ".join(translation))

'''
Translation: nous sommes <eos>
'''
```
## æ€»ç»“

1. `seq2seq` æ˜¯ä¸€ç§ä¸Šå±‚æ¨¡å‹æ¶æ„ï¼Œåº”å¯¹è¾“å…¥å’Œè¾“å‡º**ä¸å®šé•¿**çš„åœºæ™¯
2. `seq2seq` åº•å±‚å¯ä»¥ç”±**ä¸åŒ**çš„æ¨¡å‹æ„æˆ
3. `seq2seq` çš„ `Context` æ˜¯ä¿å­˜äº†**ä¸Šä¸‹æ–‡ä¿¡æ¯**ï¼Œæ˜¯ `encoder` å’Œ `decoder` éƒ½å¿…é¡»èƒ½è¯†åˆ«çš„æ ¼å¼
## 

