<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>seq2seq 手工实现及原理分析</title>
    <url>/2024/08/13/%5B%E5%AD%A6%E8%82%A5AI%5D%20seq2seq%20%E6%89%8B%E5%B7%A5%E5%AE%9E%E7%8E%B0%E5%8F%8A%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="背景问题"><a href="#背景问题" class="headerlink" title="背景问题"></a>背景问题</h2><p>现实中，有一类问题是 <strong>输入输出不定长</strong> 的，比如</p>
<ol>
<li>翻译，从中文到英文</li>
<li>文生图，一段话生成一个图片</li>
<li>摘要，总结一段话的信息</li>
</ol>
<p>所以 <code>seq2seq</code> 就是为了解决这种 一串序列 生成 另外一串序列 问题的模型。</p>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p><code>seq2seq</code>，<code>sequence to sequence</code>，也有另外一种叫法 <code>encoder and decoder</code>。他是一种上层模型架构，即是组合模型，他可以由不同的底层模型来实现。</p>
<p>我们可以先看原理图。<br><em>原理图</em><br><img src="https://qiniu.oldzhangtech.com/mdpic/65e94f22-668b-4d3f-accc-57617096e7d4_cd0a100f-9d6f-44db-86c6-5061e8e974ae.jpeg"><br>从原理图中可以知道，<code>seq2seq </code>模型 有以下的特征：</p>
<ol>
<li>模型都会有一个 <code>Encoder</code> ，一个 <code>Decoder</code>，和一个 <code>Context</code></li>
<li><code>Encoder</code> 就是字面意思的 – 编码器，<code>src_input</code> 经过<code>Encoder</code> 处理，输出 <code>Context</code> 中</li>
<li>同理，<code>Decoder</code> 就是解码器，<code>tgt_input</code> 和 <code>Context</code> 经过 <code>Decoder</code> 处理, 输出 <code>tgt_output</code></li>
<li><code>Encoder</code> 和 <code>Decoder</code> 都必须能够识别 <code>Context</code><blockquote>
<p>src： source， tgt： target</p>
</blockquote>
</li>
</ol>
<p>🔥 <code>Context</code> 的组成是非常重要的，他是 <code>Encoder</code> 和 <code>Decoder</code> 是能够识别的一个介质，是链接两者的桥梁。这种介质可以是 _隐状态_，可以是 _注意力的加权计算值_，等等，这些都由底层的模型来决定的。</p>
<p>就好比国际贸易中，我们想买澳大利亚铁矿。 美元是硬通货，中间介质，ZG 和 土澳 都认美元，所以 ZG encoder 先把 RMB 转成 Dollar，给到土澳 decoder，土澳再换回自己的 澳元。</p>
<p>🔥 <strong>不定长</strong>，输入值（比如，长度是 8）在 <code>Encoder</code> 都转换成统一的 <code>Context</code>（比如，128 X 512 的 2 层神经网络），同时 输出值的长度（比如，长度是 10 ） 由 <code>Decoder</code> 和 <code>Context</code>  来决定，已经与输入值无关了。</p>
<p>同时，<code>seq2seq</code> 仅仅是上层架构，底层实现的模型是啥都可以视情况而定。比如，底层可以是 <code>RNN</code>，可以是 <code>LSTM</code>，也可以是 <code>GRU</code>， 也可以是 <code>Transformer</code>。本文例子中使用 <code>RNN</code> 来实现。</p>
<h2 id="例子-–-翻译"><a href="#例子-–-翻译" class="headerlink" title="例子 – 翻译"></a>例子 – 翻译</h2><blockquote>
<p>下面是手工实现一个基于 <code>RNN</code> 的 <code>seq2seq</code> 模型。可运行的 ipynb 文件的<a href="https://gitee.com/oldmanzhang/resource_machine_learning/blob/master/deep_learning/seq2seq.ipynb">链接</a>。</p>
</blockquote>
<h3 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h3><p>例子的目标，从有限的翻译资料中，训练出翻译的逻辑，实现从英文翻译成法文。</p>
<h3 id="分析任务"><a href="#分析任务" class="headerlink" title="分析任务"></a>分析任务</h3><blockquote>
<p>这里先不讨论字符的处理流程（清洗字符，过滤特殊字符等），所有的流程简单化，仅仅是验证模型的使用。</p>
</blockquote>
<ol>
<li>翻译是一个“分类”任务</li>
<li>这个是一个不定长的输入和输出的，所以使用 <code>seq2seq</code> 的模型</li>
<li>同时输入和输出是有时间序列的，所以底层模型使用带有记忆能力的模型，我们使用 <code>RNN</code></li>
</ol>
<p>❓ 为什么是一份分类的任务？<br>这其实是 <code>word2index</code> 的过程，每个 <code>word</code> 就是一个分类。举例：比如 输入的是英文，英文中的一共有 4000 个单词，那么输入的分类就是 4000 ；输出的是法文，法文中的一共有 2000 个单词，那么输出的分类就是 2000。</p>
<h3 id="代码结构"><a href="#代码结构" class="headerlink" title="代码结构"></a>代码结构</h3><p><img src="https://qiniu.oldzhangtech.com/mdpic/efa06cc1-1371-4e15-a0fd-0e5505739279_54099b97-f43c-47fc-ab0a-7f855bc6ad50.jpeg"><br>上图是 数据在 seq2seq 流动中串起不同组件的过程。</p>
<p><em>组件说明：</em>   </p>
<ol>
<li><code>word_index</code>，就是把单词转换成 <code>index</code></li>
<li><code>embedding</code>，就要把离散的 <code>index</code> 转换成可以计算的连续的 <code>embedding</code>，适合模型的计算</li>
<li><code>word_index</code> 和 <code>embedding</code> 正常情况是 输入和输出都不能共用的</li>
<li><code>encoder</code> 里面有 <code>embedding</code>，<code>rnn</code><ol>
<li><code>rnn</code> 输入 <code>src</code>， 输出 <code>hidden</code> 隐状态，即 <code>Context</code></li>
</ol>
</li>
<li><code>decoder</code> 里面有 <code>embedding</code>，<code>rnn</code>，<code>full_connect</code><ol>
<li><code>rnn </code><strong>循环</strong>叠加输入 <code>tgt_input</code> 和 <code>Context</code>， 输出 <code>new hidden</code>,  <code>tgt_output</code></li>
<li><code>full_connect</code> 负责把 <code>tgt_output</code> 生成真正的 <code>real_tgt_ouput</code><blockquote>
<p>了解他们的具体职责后再去看他们的代码就清晰多了</p>
</blockquote>
</li>
</ol>
</li>
</ol>
<p><em>代码片段分析</em>  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Define the Encoder RNN</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderRNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderRNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.hidden_size = hidden_size</span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(input_size, hidden_size)</span><br><span class="line">        <span class="variable language_">self</span>.rnn = nn.RNN(hidden_size, hidden_size, batch_first=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_seq, hidden</span>):</span><br><span class="line">        <span class="comment"># 内部进行 embedding</span></span><br><span class="line">        <span class="comment"># 传入的是 input_indices</span></span><br><span class="line">        embedded = <span class="variable language_">self</span>.embedding(input_seq)</span><br><span class="line">        output, hidden = <span class="variable language_">self</span>.rnn(embedded, hidden)</span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_hidden</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, <span class="number">1</span>, <span class="variable language_">self</span>.hidden_size)</span><br></pre></td></tr></table></figure>
<p>上面是 <code>encoder</code> 的代码，作用就是：</p>
<ol>
<li><code>src_input</code> 转成 <code>embedding</code></li>
<li><code>rnn</code> 把 <code>embedding</code> 转成 <code>hidden</code>，即 <code>Context</code></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Define the Decoder RNN</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderRNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderRNN, ).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.hidden_size = hidden_size</span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(output_size, hidden_size)</span><br><span class="line">        <span class="variable language_">self</span>.rnn = nn.RNN(hidden_size, hidden_size, batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.out = nn.Linear(hidden_size, output_size)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_seq, hidden</span>):</span><br><span class="line">        <span class="comment"># 内部进行 embedding</span></span><br><span class="line">        <span class="comment"># 传入的是 input_indices</span></span><br><span class="line">        embedded = <span class="variable language_">self</span>.embedding(input_seq)</span><br><span class="line">        output, hidden = <span class="variable language_">self</span>.rnn(embedded, hidden)</span><br><span class="line">        <span class="comment"># 就是 全链接层 从 hidden -》 output_feature</span></span><br><span class="line">        output = <span class="variable language_">self</span>.out(output.squeeze(<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_hidden</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, <span class="number">1</span>, <span class="variable language_">self</span>.hidden_size)</span><br></pre></td></tr></table></figure>
<p>上面是 <code>dncoder</code> 的代码，与 <code>encoder </code>比较多了一个 <code>full connect</code> 使用</p>
<ol>
<li><code>tgt_input</code> 转成 <code>embedding</code></li>
<li><code>rnn</code> 把 <code>embedding</code> 转成 <code>hidden</code> 和 <code>output</code></li>
<li><code>full conect </code>再把 <code>output</code> 转成 <code>output_feature</code></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Define the Seq2Seq model</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2Seq</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder</span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2Seq, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># [1]</span></span><br><span class="line">        <span class="variable language_">self</span>.encoder = encoder</span><br><span class="line">        <span class="variable language_">self</span>.decoder = decoder</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src_seq, tgt_seq, teacher_forcing_ratio=<span class="number">0.5</span></span>):</span><br><span class="line">        batch_size = src_seq.size(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># count of words [[0, 1, 2, 9]]</span></span><br><span class="line">        max_len = tgt_seq.size(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 11</span></span><br><span class="line">        tgt_vocab_size = <span class="variable language_">self</span>.decoder.out.out_features</span><br><span class="line">        <span class="comment"># 有 11 个隐状态，就是 target 中的唯一值</span></span><br><span class="line">        outputs = torch.zeros(batch_size, max_len, tgt_vocab_size)</span><br><span class="line">        </span><br><span class="line">        encoder_hidden = <span class="variable language_">self</span>.encoder.init_hidden()</span><br><span class="line">        <span class="comment"># encoder 的作用是 输出 hidden， output 就没有什么意义了</span></span><br><span class="line">        <span class="comment"># [2]</span></span><br><span class="line">        encoder_output, encoder_hidden = <span class="variable language_">self</span>.encoder(src_seq, encoder_hidden)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># tgt_seq 作用，就是取得第一个 &lt;sos&gt; token</span></span><br><span class="line">        decoder_input = tgt_seq[:, <span class="number">0</span>].unsqueeze(<span class="number">1</span>)  <span class="comment"># Start with &lt;sos&gt;</span></span><br><span class="line">        decoder_hidden = encoder_hidden</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># tgt_seq 作用，截取输出的长度</span></span><br><span class="line">        <span class="comment"># 不取 0，是因为 “0“ index 是一个 &lt;sos&gt;</span></span><br><span class="line">        <span class="comment"># [3]</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, max_len):</span><br><span class="line">            <span class="comment"># [4]</span></span><br><span class="line">            decoder_output, decoder_hidden = <span class="variable language_">self</span>.decoder(decoder_input, decoder_hidden)</span><br><span class="line">            <span class="comment"># decoder_output shape (1,11)，其实是一个多分类的问题</span></span><br><span class="line">            <span class="comment"># 与 outputs[:, t] = decoder_output 是一样的，因为 batch_size 恒等于 1，所以暂时影响不大，但是实际应用中，应该要改成对应的 batch</span></span><br><span class="line">            outputs[:, t, :] = decoder_output</span><br><span class="line">            top1 = decoder_output.argmax(<span class="number">1</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 这里是取巧了，teacher_forcing_ratio 是取巧了。</span></span><br><span class="line">            <span class="comment"># decode_input_t+1 有时是 decode_output_t， 有时是 real_target_seq_t</span></span><br><span class="line">            <span class="comment"># [5]</span></span><br><span class="line">            decoder_input = tgt_seq[:, t].unsqueeze(<span class="number">1</span>) <span class="keyword">if</span> random.random() &lt; teacher_forcing_ratio <span class="keyword">else</span> top1</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<p>上面的代码是 <code>seq2seq</code> 模型的定义。</p>
<p><em>训练过程</em><br>可以检查数据在这个模型中流动如下：</p>
<ol>
<li>[1] 里面包含了一个 <code>encoder</code> 和 <code>decoder</code></li>
<li>[2] <code>forword</code> 时,  <code>encoder</code> 转换 <code>src_input</code> 成 <code>hidden</code></li>
<li>[3] 开始 <code>decoder</code> 循环，最大长度是 <code>max_len</code>。初始化即是： <code>decoder_input = “&lt;sos&gt; index“</code>，<code>decoder_hidden = encoder_hidden</code></li>
<li>[4] <code>decoder</code> 输出是 <code>output_index</code> + <code>new_hidden</code></li>
<li>[5] <code>decoder_input+= output_index</code>, <code>decoder_hidden += new_hidden</code> 叠加后再走步骤 [3] 循环</li>
</ol>
<p>💡 <code>teacher_forcing</code> 是什么？<br>就是训练的时候，有一定的概率输出是 <em>真实值</em> 而不是 _预测值_。就能是模型更加快的收敛，加速模型的学习。但是过于依赖 _真实值_，就会导致泛化能力差。<code>teacher_forcing_ratio</code> 就可以调整阈值。</p>
<p><em>推理过程</em>  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input_seq = torch.tensor(indices, dtype=torch.long).unsqueeze(<span class="number">0</span>)  <span class="comment"># (1, seq_len)</span></span><br><span class="line"><span class="comment"># 全部的input，都转成 hidden</span></span><br><span class="line">encoder_hidden = model.encoder.init_hidden()</span><br><span class="line"><span class="comment"># encoder 和 decoder 的使用是分开的、</span></span><br><span class="line"><span class="comment"># [1]</span></span><br><span class="line">encoder_output, encoder_hidden = model.encoder(input_seq, encoder_hidden)</span><br><span class="line"><span class="comment"># [2]</span></span><br><span class="line">decoder_input = torch.tensor([[fra_word2idx[<span class="string">&#x27;&lt;sos&gt;&#x27;</span>]]], dtype=torch.long)  <span class="comment"># Start token</span></span><br><span class="line">decoder_hidden = encoder_hidden</span><br><span class="line"></span><br><span class="line">translated_sentence = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># [3]</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_length):</span><br><span class="line">    <span class="comment"># decoder_input 是逐步的累加的，就是 word1+word2+word3...</span></span><br><span class="line">    <span class="comment"># 第一个 decoder_hidden 是 encoder_hidden</span></span><br><span class="line">    <span class="comment"># 从第二个开始，就是循环得到 decoder_hidden 不停的传入</span></span><br><span class="line">    <span class="comment"># encoder 和 decoder 的使用是分开的</span></span><br><span class="line">    <span class="comment"># [4]</span></span><br><span class="line">    decoder_output, decoder_hidden = model.decoder(decoder_input, decoder_hidden)</span><br><span class="line">    top1 = decoder_output.argmax(<span class="number">1</span>).item()</span><br><span class="line">    <span class="comment"># &quot;&lt;UNK&gt;&quot;, which stands for “unknown.”</span></span><br><span class="line">    <span class="comment"># [5]</span></span><br><span class="line">    translated_word = fra_idx2word.get(top1, <span class="string">&quot;&lt;UNK&gt;&quot;</span>)</span><br><span class="line">    translated_sentence.append(translated_word)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [6]</span></span><br><span class="line">    <span class="keyword">if</span> translated_word == <span class="string">&#x27;&lt;eos&gt;&#x27;</span>:  <span class="comment"># End of sentence</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    decoder_input = torch.tensor([[top1]], dtype=torch.long)  <span class="comment"># Next input token</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> translated_sentence</span><br></pre></td></tr></table></figure>
<p><em>推理过程</em> 和 _训练过程_，具体原理一致。 有以下的差异点需要注意：</p>
<ol>
<li>如何定义开始输出的标志</li>
<li>如何定义结束输出的标志</li>
<li>如何定义不认识字符的标志</li>
</ol>
<p>代码分析：</p>
<ol>
<li>[1] 单独使用 <code>seq2seq&#39;s encoder</code>，且 <em>一次性</em> 生成 <code>encoder hidden</code></li>
<li>[2] <code>decoder_input</code> 初始化，以  ‘<sos>‘ 开头，标志开始输出</li>
<li>[3] <code>decoder</code> 开始循环<ol>
<li>[4] 单独使用 <code>seq2seq&#39;s decoder</code>, 输出 <code>ouput </code>和 <code>new_hidden</code></li>
<li>[5] 碰到不认识的分类，就使用 ‘<UNK>‘取代</li>
<li>[6] 如果遇到 ‘<eos>‘ 字符就直接结束循环</li>
<li>回到 [3] 继续循环</li>
</ol>
</li>
</ol>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练结果</span></span><br><span class="line">Epoch: <span class="number">0</span>, Loss: <span class="number">2.820215034484863</span></span><br><span class="line">Epoch: <span class="number">100</span>, Loss: <span class="number">1.0663029670715332</span></span><br><span class="line">Epoch: <span class="number">200</span>, Loss: <span class="number">1.1840879678726197</span></span><br><span class="line">Epoch: <span class="number">300</span>, Loss: <span class="number">1.224123215675354</span></span><br><span class="line">Epoch: <span class="number">400</span>, Loss: <span class="number">1.0645174384117126</span></span><br><span class="line">Epoch: <span class="number">500</span>, Loss: <span class="number">1.061875820159912</span></span><br><span class="line">Epoch: <span class="number">600</span>, Loss: <span class="number">1.0744179487228394</span></span><br><span class="line">Epoch: <span class="number">700</span>, Loss: <span class="number">1.0767890691757203</span></span><br><span class="line">Epoch: <span class="number">800</span>, Loss: <span class="number">1.099305510520935</span></span><br><span class="line">Epoch: <span class="number">900</span>, Loss: <span class="number">1.1019723176956178</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">test_sentence = [<span class="string">&quot;i&quot;</span>, <span class="string">&quot;am&quot;</span>]</span><br><span class="line">translation = translate(model, test_sentence)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Translation:&quot;</span>, <span class="string">&quot; &quot;</span>.join(translation))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Translation: nous sommes &lt;eos&gt;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol>
<li><code>seq2seq</code> 是一种上层模型架构，应对输入和输出<strong>不定长</strong>的场景</li>
<li><code>seq2seq</code> 底层可以由<strong>不同</strong>的模型构成</li>
<li><code>seq2seq</code> 的 <code>Context</code> 是保存了<strong>上下文信息</strong>，是 <code>encoder</code> 和 <code>decoder</code> 都必须能识别的格式</li>
</ol>
<h2 id=""><a href="#" class="headerlink" title=""></a></h2>]]></content>
      <categories>
        <category>学肥AI</category>
      </categories>
      <tags>
        <tag>学肥AI</tag>
        <tag>seq2seq</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>手写 Attention 注意力机制 及理解</title>
    <url>/2024/08/16/%5B%E5%AD%A6%E8%82%A5AI%5D%20%E6%89%8B%E5%86%99%20Attention%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%20%E5%8F%8A%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<h2 id="背景问题"><a href="#背景问题" class="headerlink" title="背景问题"></a>背景问题</h2><p><code>RNN</code> 和 各种变体 <code>RNN</code> 中 <code>LSTM</code>&#x2F;<code>GRU</code> 都存在一个问题，就是如何解决 长距离信息的感知。<code>RNN</code> 的解决办法是加大 <code>sequence</code>，更长的窗口记得更加久远的信息；<code>LSTM</code> 和 <code>GRU</code> 就是把记忆设置成不同的权重，对重要的信息加大权重。<code>Attention</code> 又是另外一个角度，去解决这个问题。</p>
<h2 id="Attention-是什么"><a href="#Attention-是什么" class="headerlink" title="Attention 是什么"></a>Attention 是什么</h2><p><code>Attention</code> 中文是注意力机制，是对某个事物的某部分的关注点。</p>
<p>从人脑的角度想，时间和资源都是有限的，所以只能在一定的限度内去关注某些部分。比如看到一群人的照片，我们自然去找美女；看一个美女的照片，我们自然去看美女的眼睛。我们为什么会不自主的去看这些部分而不是看全景呢？因为我们的注意力资源是有限的，我们只对关注点高的部分感兴趣。<br>这是属于在我们人脑里面的注意力机制。<br>从模型的角度想，用数学将他们建模，他们应该是注意力得分最高的部分，也就是模型可以重点关注的地方。</p>
<p>总结，上述就是 <code>Attention Score</code> 的基本的理解。谁得分高，谁就可以得到更加多的关注。</p>
<p>下面把 <code>Attention Score</code> 用在一段话的理解上。</p>
<p>我们使用一句话作为例子：姚明，他爸有 2 米高，他从小的数学成绩并不好，经常旷课，他爱吃苹果，他血型是 A。爱玩游戏，他最喜欢的游戏就是塞尔达传说，他还经常逃课去玩；有时候他会打一下篮球和兵乓球，他最喜欢的运动是跑步。他现在身高是 2.13 米，体重是 200 斤，鞋子要穿 48 码的。<br>请问：他为什么可以长这么高。</p>
<p>从人脑的角度，“姚明为什么这么高”，很自然的想到，他父母高，基因好。之所以有这个判断，是因为问题，和 “父母高”这个信息是 <strong>关联度最高</strong>的。<br>从模型的角度想，用数学将他们建模，即 <em>问题</em> 和 <em>信息的某个部分</em> ，计算的<code>Score</code> 得分最高，所以他们的关联度最高，即他们具有逻辑相关性。</p>
<p>总结，<code>Attention 机制</code> 在自然语言理解上，即相互的 <code>Score</code> 越高分，即相互的关联性就越大，即他们具有逻辑性。</p>
<blockquote>
<p>题外话： 这个有点像 <strong>逻辑</strong>。<br>我们大脑是神经元构成的。神经元，即是 给一个高电平就是 1 ，给一个低电平就是 0 的触角；这个触角，又可以触发另外的神经元。这种就是 “一生二，二生三，三生万物”的体现。因为这种触角的触发，就在我们的大脑里面形成了 <strong>逻辑</strong>。<br><code>Attention Score</code>，有可能构成了模型领域中的 _基础触角_，最后形成了模型领域中 <strong>逻辑</strong>。</p>
</blockquote>
<h2 id="分概念讲解"><a href="#分概念讲解" class="headerlink" title="分概念讲解"></a>分概念讲解</h2><p>从上面的例子可以简单的梳理出 <code>Attention 机制</code>。但是落到细节里面，<code>Attention Score</code> 怎么计算，就是从实际的数学模型的角度去说明了。下面就 <code>Attention Score</code> 怎么去计算为线头，来讲解不同概念。</p>
<blockquote>
<p>下面是一步一步的说明 Attention 机制 的各种组件，和他们能够解决到什么问题。</p>
</blockquote>
<h3 id="QKV-机制"><a href="#QKV-机制" class="headerlink" title="QKV 机制"></a>QKV 机制</h3><blockquote>
<p>Query Key Value</p>
</blockquote>
<p>如果对上述的那句话分成不同的片段：“他爸有 2 米高” seg1，“他从小的数学成绩并不好” seg2，“经常旷课” seg3，“他爱吃苹果” seg4，“他血型是 A” seg5，每个片段都携带了一定量的信息，我们统称他们携带的信息是 <strong>隐状态</strong> <strong>hidden</strong>。对应着，片段的隐状态就是 h1, h2…h5。</p>
<p>面对 “姚明为什么这么高” 问题的时候，自然的就认为 <code>seg1</code> 的关联性是最高的。但是如果面对 “他三角函数不懂”问题的时候，显然是 <code>seg2</code> 的关联性是最高的。<br>所以的出， 同一个<code>query</code> 对应不同的 <code>segment</code> 的，有不同的 <code>Attention Score</code>；不同的 <code>query</code> 都应该对 同一批的 <code>segment</code> 有不同的 <code>Attention Score</code>。</p>
<p><img src="https://qiniu.oldzhangtech.com/mdpic/27d4a47e-6390-46e1-9cfc-cf807a964c95_1b303e9c-fe57-42f2-80ce-1b15069c47fc.jpeg"><br>如上图所示，就是连线的宽度不同，显示了 <code>query</code> 和 不同 <code>segment</code> 之间的相关性，即不同的得分。</p>
<p>如果基于传统的 <code>RNN</code> 去表达 <code>Attention</code>，有一定的局限性。 <code>RNN</code> 的计算隐状态 <code>hidden</code> 都是不变的，即每个信息（片段）仅仅只有一个维度，只有一个值。<br>所以 <code>Attention 机制</code> 将 <code>hidden</code> 分拆成 <code>key</code> 和 <code>value</code>，且 <code>query</code> 循环和所有的 <code>key</code> 和 <code>value</code> 计算后才得到 <code>Score</code>。</p>
<p><img src="https://qiniu.oldzhangtech.com/mdpic/ed7f2a6d-58cc-4d66-ab04-87e683be4ded_b6102f89-59fd-47a1-94f4-4c0096fa84d4.jpeg"><br>上图简化出来的公式就是 <code>Attention Score</code> &#x3D; $Attention(QW^Q,KW^K,VW^V)$ ，就是 <strong>注意力评分公式</strong> 了。</p>
<p>这种把 <code>hidden</code> 拆分成 <code>key</code> 和 <code>value</code>，并且结合  <code>query</code> 计算就是 <code>QKV 机制</code>（ query key value 机制）。</p>
<h3 id="多头注意力机制"><a href="#多头注意力机制" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h3><blockquote>
<p>Multiple Head Attention</p>
</blockquote>
<p>从上述的 <code>QKV 机制</code> 知道，<code>query</code> 就是根据兴趣点，触发对片段计算评分。<br>如果仅仅是一个 <code>query</code>，其实和 单一使用 <code>RNN hidden</code> 是没有区别的；但是如果增加多几个 <code>query</code>，就可以对信息进行不同维度的分层，也就是 <strong>多头</strong> 的意思。</p>
<p>🔥 多头注意力机制，就是字如其名，多个 <code>head</code>，多个 <code>query</code>； 一个 <code>head</code> 就是 一个 <code>query</code>。</p>
<p>比如，“姚明为什么这么高”，“他三角函数不懂” 这两个 <code>query</code> 都同时对原信息进行统计评分，就可以得到不同 <code>segment</code> 对应的 <code>Attention Score</code>。</p>
<p>🔥 有点像是  <code>CNN</code> 的卷积核，可以抽取不同的特征维度。</p>
<p><img src="https://qiniu.oldzhangtech.com/mdpic/d759a94d-fe06-4a2c-baee-e3600f458cd8_389bd982-7410-49f2-9f81-577b6901d74c.jpeg"><br><code>query</code>, <code>key</code>, <code>value</code> 因为不同的 <code>head</code> 都带有自己的（$W^Q W^K W^V$） 矩阵进行学习，这样就能够带来更加多的学习性。</p>
<h3 id="自注意力机制"><a href="#自注意力机制" class="headerlink" title="自注意力机制"></a>自注意力机制</h3><blockquote>
<p>Self Attention</p>
</blockquote>
<p>有时候，一句话中，已经蕴含了逻辑。比如上面的“姚明描述”，就算没有提问，都可以从自身的句子中建立了关联。</p>
<p>比如，“他从小的数学成绩并不好” <code>seg2</code> 和 “经常旷课” <code>seg3</code>，他们两个片段就有非常强的相关性，他们 <code>Attention Score</code> 的得分就高；同理“经常旷课” <code>seg3</code>，“他爱吃苹果” <code>seg4</code>，他们的得分就不高。</p>
<p>🔥 所以当 <code>Query Key Value</code> 都是自己的，就是 <strong>自注意力机制</strong>，对自己的信息片段建立 <code>Attention Score</code>。</p>
<p><img src="https://qiniu.oldzhangtech.com/mdpic/0cdf99e8-066b-4390-a980-6c5ac39ce0b2_e1aab5d8-ebd1-4c02-8f37-6bed0a48f488.jpeg"><br>上图可以知道，越粗的线，就是他们的 <code>attention score</code> 更加的高分；且这是自身片段和片段之间的评分。就是自注意力机制的体现。</p>
<h3 id="注意力得分代码部分"><a href="#注意力得分代码部分" class="headerlink" title="注意力得分代码部分"></a>注意力得分代码部分</h3><p><code>Attention Score</code>，到底他们是如何计算的呢？<br>下面是简易自注意力得分的代码片段。里面已经包含了 <strong>QKV 机制</strong>，<strong>多头注意力机制</strong>，和 <strong>自注意力机制</strong>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># [0]</span></span><br><span class="line">query_f = nn.Linear(d_model, d_model)</span><br><span class="line">key_f = nn.Linear(d_model, d_model)</span><br><span class="line">value_f = nn.Linear(d_model, d_model)</span><br><span class="line">fc = nn.Linear(d_model, d_model)</span><br><span class="line"><span class="comment"># [3]</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span><br><span class="line">    batch_size = query.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">transform</span>(<span class="params">x</span>):</span><br><span class="line">        <span class="keyword">return</span> x.view(batch_size, -<span class="number">1</span>, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># [1] 对 query，key，value 进行全链接的转换</span></span><br><span class="line">    query = transform(query_f(query))</span><br><span class="line">    key = transform(key_f(key))</span><br><span class="line">    value = transform(value_f(value))</span><br><span class="line">    <span class="comment"># [2] 封装的方法</span></span><br><span class="line">    attn_output, attn = ScaledDotProductAttention(<span class="variable language_">self</span>.d_k)(query, key, value, mask)</span><br></pre></td></tr></table></figure>
<p>_代码解析_：  </p>
<ol>
<li>[0] query_f，key_f， value_f 定义 全链接网络，用于学习的参数</li>
<li>[1] query，key，value 进行全链接的转换，这里有参数的学习</li>
<li>[2] 封装的方法，可以计算出最后的 <code>Attention Sore</code>，里面已经包含 点积&#x2F;softmax 等计算</li>
<li>[3] 如果 query, key, value 都是同一个变量的时候，那么就是一个 <strong>多头的自注意力机制</strong> 的计算公式</li>
</ol>
<p> 多头自注意力机制的数学表达式：<br>$head_i&#x3D;Attention(QW_i^Q,KW_i^K,VW_i^V)$ （ $W_i^Q W_i^K W_i^V$ 都是不同的全链接网络 ）</p>
<h3 id="例子说明"><a href="#例子说明" class="headerlink" title="例子说明"></a>例子说明</h3><p>可运行的 ipynb 文件<a href="https://gitee.com/oldmanzhang/resource_machine_learning/blob/master/deep_learning/attention.ipynb">链接</a>。<br>任务：输入数据，计算数据之间的注意力分数，并且可以视觉化数据之间的关注度。</p>
<p><em>代码解析</em>  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input_seq = torch.tensor([</span><br><span class="line">    [<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">0.0</span>],  <span class="comment"># Token 1</span></span><br><span class="line">    [<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>],  <span class="comment"># Token 2</span></span><br><span class="line">    [<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>],  <span class="comment"># Token 3</span></span><br><span class="line">    [<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>],  <span class="comment"># Token 4</span></span><br><span class="line">    [<span class="number">1.0</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.0</span>]   <span class="comment"># Token 5</span></span><br><span class="line">], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Dimensions</span></span><br><span class="line">d_k = input_seq.shape[<span class="number">1</span>]  <span class="comment"># Embedding dimension (4 in this case)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Query, Key, and Value are all set to the input sequence (self-attention)</span></span><br><span class="line">query = input_seq</span><br><span class="line">key = input_seq</span><br><span class="line">value = input_seq</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate attention scores (scaled dot-product)</span></span><br><span class="line"><span class="comment"># [1]</span></span><br><span class="line">scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / torch.sqrt(torch.tensor(d_k))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Attention scores:\n&quot;</span>, scores)</span><br><span class="line"><span class="comment"># Apply softmax to get attention weights</span></span><br><span class="line"><span class="comment"># [2]</span></span><br><span class="line">attention_weights = F.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Attention scores afert softmax:\n&quot;</span>, attention_weights)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate the output as a weighted sum of the values</span></span><br><span class="line"><span class="comment"># [3]</span></span><br><span class="line">output = torch.matmul(attention_weights, value)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Attention Weights:\n&quot;</span>, attention_weights)</span><br></pre></td></tr></table></figure>

<ol>
<li><p>[1] 简单使用对 query key 进行 点积 来计算分数</p>
<blockquote>
<p>这里使用了简化版本，<strong>没有</strong> $W^q$ $W^k$ 的矩阵学习</p>
</blockquote>
</li>
<li><p>[2] softmax score，全部值归一到 (0,1) 的值中</p>
</li>
<li><p>[3] score 和 value 相乘，得到最后的 weights，就是最后的结果</p>
<blockquote>
<p>这里使用了简化版本，<strong>没有</strong> $W^v$  的矩阵学习</p>
</blockquote>
</li>
</ol>
<p><em>运行结果</em>  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Attention scores:</span></span><br><span class="line"><span class="string"> tensor([[1.0000, 0.0000, 0.5000, 0.5000, 0.7500],</span></span><br><span class="line"><span class="string">        [0.0000, 1.0000, 0.5000, 0.5000, 0.2500],</span></span><br><span class="line"><span class="string">        [0.5000, 0.5000, 1.0000, 0.0000, 0.7500],</span></span><br><span class="line"><span class="string">        [0.5000, 0.5000, 0.0000, 1.0000, 0.2500],</span></span><br><span class="line"><span class="string">        [0.7500, 0.2500, 0.7500, 0.2500, 0.7500]])</span></span><br><span class="line"><span class="string">Attention scores afert softmax:</span></span><br><span class="line"><span class="string"> tensor([[0.2976, 0.1095, 0.1805, 0.1805, 0.2318],</span></span><br><span class="line"><span class="string">        [0.1205, 0.3275, 0.1986, 0.1986, 0.1547],</span></span><br><span class="line"><span class="string">        [0.1805, 0.1805, 0.2976, 0.1095, 0.2318],</span></span><br><span class="line"><span class="string">        [0.1986, 0.1986, 0.1205, 0.3275, 0.1547],</span></span><br><span class="line"><span class="string">        [0.2374, 0.1440, 0.2374, 0.1440, 0.2374]])</span></span><br><span class="line"><span class="string">Attention Weights:</span></span><br><span class="line"><span class="string"> tensor([[0.2976, 0.1095, 0.1805, 0.1805, 0.2318],</span></span><br><span class="line"><span class="string">        [0.1205, 0.3275, 0.1986, 0.1986, 0.1547],</span></span><br><span class="line"><span class="string">        [0.1805, 0.1805, 0.2976, 0.1095, 0.2318],</span></span><br><span class="line"><span class="string">        [0.1986, 0.1986, 0.1205, 0.3275, 0.1547],</span></span><br><span class="line"><span class="string">        [0.2374, 0.1440, 0.2374, 0.1440, 0.2374]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p><img src="https://qiniu.oldzhangtech.com/mdpic/6d9f194c-b314-4af9-bb93-dd782176ad6a_169e0c5f-3757-4035-b6eb-aa505bd920b9.png" alt="image.png"><br>从图上可以看到，注意力的热图，表示每个 token 之间的注意力的关系。 Y 轴是 <code>Query Token</code>，X 轴是 <code>Key Token</code>。图中每一行中越深色的方块，就代表 <code>Query</code> 在这行中的  <code>Key</code> 的得分最高。<br>比如，第一行，<code>query1</code> 对 <code>key1</code>（自己）的颜色最深，说明每个 <code>Token</code> 都应该与自己的关联度高；第 5 行，<code>query5</code> 对 <code>key1</code>，<code>key3</code>，<code>key5</code> 的得分一样且颜色最深，说明 <code>query5</code> 的关联与 <code>key1</code>，<code>key3</code>，<code>key5</code> 相一样。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><code>Attention 机制</code> 的优势  </p>
<ol>
<li>关注你想要的信息，解决了长序列的问题</li>
<li>可以有多维的角度去理解数据</li>
<li>其中蕴含了逻辑</li>
</ol>
<p><code>Attention 机制</code> 是 <code>Transformer</code> 的基础，所有所有的 <code>NLP</code> 中打开了新的一扇窗。</p>
<h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p><a href="https://zh-v2.d2l.ai/chapter_attention-mechanisms/attention-cues.html#id4">Dive into deep learning</a></p>
]]></content>
      <categories>
        <category>学肥AI</category>
      </categories>
      <tags>
        <tag>学肥AI</tag>
        <tag>深度学习</tag>
        <tag>Attention</tag>
      </tags>
  </entry>
  <entry>
    <title>手写 LSTM 和 GRU 且原理分析</title>
    <url>/2024/08/09/%5B%E5%AD%A6%E8%82%A5AI%5D%20%E6%89%8B%E5%86%99%20LSTM%20%E5%92%8C%20GRU%20%E4%B8%94%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="背景问题"><a href="#背景问题" class="headerlink" title="背景问题"></a>背景问题</h2><p><code>RNN</code> 是解决时间序列数据的模型。但是他无法解决时间步过长而无法记住长期信息的这个问题。从而诞生了很多 <code>RNN</code> 的变种模型来解决这些问题。我们今天来看下他们的原理和手写他们的实现。</p>
<h2 id="复杂-RNN"><a href="#复杂-RNN" class="headerlink" title="复杂 RNN"></a>复杂 RNN</h2><p>常规的 <code>RNN</code> 的逻辑图如下：<br><img src="https://qiniu.oldzhangtech.com/mdpic/95f069d6-06bc-4df6-bbc8-89dc1ab54513_145485d3-8e45-4726-b6e8-fcf46df277fa.jpeg"></p>
<p>代码如下：<br>$h_{t} &#x3D; f(h_{t-1},x_t)$<br>$o_t &#x3D; g(h_t)$ </p>
<p>为了解决 传统 <code>RNN</code> 的问题，就有了以下的思路：  </p>
<ol>
<li>在不同的转换过程（每个连线）中增加多 一些轻量级的转换，增加记忆度</li>
<li>加上残差层</li>
<li>增加多更多的输出层，先抽取低维特征，再抽取高维特征的方法</li>
</ol>
<p>各种的变体的 <code>RNN</code> 的架构图如下：<br><img src="https://qiniu.oldzhangtech.com/mdpic/e58e3b3d-03da-4c17-bd57-fdde80d71a73_184ba76b-3fd9-45d1-b587-d4dd986b95fc.jpeg"></p>
<p>其中最优秀的两种变体模型就是 <code>LSTM</code> 和 <code>GRU</code> 的模型。下面分别对下面的模型进行介绍。</p>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p><code>LSTM</code>（Long Short-Term Memory）全称 长短期记忆。简单的说，就是记忆分长期和短期的。长期的记忆，可以长时间存活；同理，短期记忆生存时间就比较短。</p>
<p>🔥 <code>RNN</code> 记忆是存在每个时间步的隐状态中的，随着时间的推移，会“遗忘”时间长的隐状态，即权重逐步减少； <code>LSTM</code> 就是 针对重要的记忆，拿个记事本记住，让他不会随着时间的推移而忘记了。</p>
<p><code>LSTM</code> 与人类的日记习惯一致，每天记录到日记的事情都是重要的事情，但是不会 24 小时每分每秒的事情都记住。某年某月某日中了双色球头奖（长期记忆），都会记录下来，多年后再翻看，都会记忆犹新；同时，489 天前的晚饭吃了什么（短期记忆），大体不会出现在你日记本内，当然也不会在你的脑海内。</p>
<blockquote>
<p>记住了原理就可以，不需要过多的细节</p>
</blockquote>
<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p><em>原理图</em><br><img src="https://qiniu.oldzhangtech.com/mdpic/6cf90fbb-3bef-4376-afa2-49b857b98e89_9a2d9ced-e5a5-4497-95db-0418b57fc8da.jpeg"><br>对比 <code>RNN</code>，<code>LSTM</code> 就可以对某些记忆认为是长期的且重要的，进行加权，让他的权重不减弱。</p>
<p><em>逻辑说明</em><br>这里出现了一个新的概念：<code>Cell 记忆</code> 他是包含在 <code>LSTMCell</code> 里面一个内置的记忆体，他就是用于记住哪些 <code>Long Term Memory</code>。 </p>
<p><code>LSTMCell</code>  就是 <code>LSTMModel</code> 的基础的组件，下图就是 <code>LSTMCell</code> 的原理图。整个 <code>Cell</code>  有 3 个门来控制记忆，包括了 遗忘门&#x2F;输入门&#x2F;输出门，由他们来控制 输入&#x2F;输出&#x2F;隐状态&#x2F;记忆 之间的关系。<br><img src="https://qiniu.oldzhangtech.com/mdpic/bdf5c0f7-e18f-401e-b7b5-c43ddab01baf_dbd5558a-4c81-447c-8bb6-a83b18f14a7c.png" alt="image.png"></p>
<blockquote>
<p>我们直接 dive deep into code，对照这个原理图，会更加容易理解。</p>
</blockquote>
<h3 id="组件"><a href="#组件" class="headerlink" title="组件"></a>组件</h3><p>下面是节选<a href="https://gitee.com/oldmanzhang/resource_machine_learning/blob/master/deep_learning/lstm.ipynb">例子</a>中的 <strong>手写LSTM</strong> 的 <code>LSTM Cell</code> 代码片段。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LSTMCell</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(LSTMCell, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.hidden_size = hidden_size</span><br><span class="line">        <span class="comment"># 输入门</span></span><br><span class="line">        <span class="variable language_">self</span>.input_gate = nn.Linear(input_size + hidden_size, hidden_size)</span><br><span class="line">        <span class="comment"># 遗忘门</span></span><br><span class="line">        <span class="variable language_">self</span>.forget_gate = nn.Linear(input_size + hidden_size, hidden_size)</span><br><span class="line">        <span class="comment"># 输出门</span></span><br><span class="line">        <span class="variable language_">self</span>.output_gate = nn.Linear(input_size + hidden_size, hidden_size)</span><br><span class="line">        <span class="comment"># 本 Cell 的记忆门</span></span><br><span class="line">        <span class="variable language_">self</span>.cell_gate = nn.Linear(input_size + hidden_size, hidden_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, interState</span>):</span><br><span class="line">        h_prev, c_prev = interState</span><br><span class="line"></span><br><span class="line">        combined = torch.cat((x, h_prev), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        i_t = torch.sigmoid(<span class="variable language_">self</span>.input_gate(combined))</span><br><span class="line">        f_t = torch.sigmoid(<span class="variable language_">self</span>.forget_gate(combined))</span><br><span class="line">        o_t = torch.sigmoid(<span class="variable language_">self</span>.output_gate(combined))</span><br><span class="line">        c_tilde = torch.tanh(<span class="variable language_">self</span>.cell_gate(combined))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 主要逻辑，下面两行</span></span><br><span class="line">        c_t = f_t * c_prev + i_t * c_tilde</span><br><span class="line">        h_t = o_t * torch.tanh(c_t)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> h_t, (h_t, c_t)</span><br></pre></td></tr></table></figure>
<p>请结合注释和原理图，进行阅读</p>
<p><em>总结</em>   </p>
<ol>
<li>上述逻辑 都是简单的 <strong>全链接</strong> 和 <strong>激活函数</strong> 构成，所以理解原理是最重要的，内部都是组装而已</li>
<li>因为是手写 <code>LSTMCell</code>，简化了逻辑，仅仅是针对 单个数据 + 记忆 处理；sequence 的数据 就是在训练的时候，进行循环；batch 的概念，当然也是没有的。<blockquote>
<p>⚠️ 这里影响到了 1. X, y 的数据结构； 2. 模型内的 forward 的处理方法；3. train 的方法。可以对比文件后面的  nn.LSTM 的解决方法，来一起服用。<br>最好的方式当然是把 手写 LSTMCell 改写成 (batch, sequence, input_feature) X 数据结构 都适用的代码。</p>
</blockquote>
</li>
</ol>
<h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>例子目标： 输入 5 个数字，可以预测下一个数据是多少。</p>
<p><em>分析过程</em>  </p>
<ol>
<li>是 回归问题</li>
<li>“5 个数字” 是一个有连续先后关系的输入，所以使用类 RNN 的模型</li>
<li>定义 RNN 的时候 input_feature 和 output_feature 都是 1</li>
</ol>
<p>详细的代码可以直接访问 ipynb 文件<a href="https://gitee.com/oldmanzhang/resource_machine_learning/blob/master/deep_learning/lstm.ipynb">链接</a>。里面有 手动LSTM 和 nn.LSTM 两种不同的实现方案。</p>
<p><em>训练后的结果</em>   </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line">epoch: <span class="number">400</span> loss: <span class="number">0.01360714</span></span><br><span class="line">epoch: <span class="number">450</span> loss: <span class="number">0.00120955</span></span><br><span class="line">epoch: <span class="number">499</span> loss: <span class="number">0.0031908983</span></span><br><span class="line"><span class="comment"># 在 500 后收敛</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入 45,46,47,48,49,50</span></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">[<span class="number">49.966102600097656</span>, <span class="number">50.8197021484375</span>, <span class="number">51.54384231567383</span>, <span class="number">52.14518737792969</span>, <span class="number">52.62910842895508</span>]</span><br></pre></td></tr></table></figure>

<h2 id=""><a href="#" class="headerlink" title=""></a></h2><h2 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h2><p><code>GRU</code>（Gated Recurrent Unit）全称 门控循环单元，就是使用 逻辑电路的思路去解决模型的问题。</p>
<p>他和 <code>LSTM</code> 一样，都是为了解决 <code>RNN</code> 的长期记忆的问题。同时比 <code>LSTM</code> 的优势是，仅仅是用了两个门 – 重置门 和 更新门。门的数量的减少，自然参数量可以进一步的减少。</p>
<blockquote>
<p>记住了原理就可以，不需要过多的细节</p>
</blockquote>
<h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p><em>原理图</em><br><img src="https://qiniu.oldzhangtech.com/mdpic/98cd28fc-f93a-49b2-873c-6a432f819079_fbdfb78f-be9e-4200-b583-2b2205b6bd64.png" alt="image.png"><br><code>GRUCell</code>  就是 <code>GRUModel</code> 的基础的组件，上图就是 <code>GRUCell</code> 的原理图。整个 <code>Cell</code>  有 2 个门来控制记忆，包括了 重置门&#x2F;更新门，由他们来控制 输入&#x2F;输出&#x2F;隐状态 之间的关系。</p>
<p>⚠️ 注意，与 <code>LSTM</code> 对比，<code>GRU</code> 没有 <strong>记忆</strong> 的概念，他的所有的记忆都是隐状态中，并且他是使用 <code>门</code> 来更新&#x2F;重置 隐状态。 </p>
<h3 id="组件-1"><a href="#组件-1" class="headerlink" title="组件"></a>组件</h3><p>下面是节选<a href="https://gitee.com/oldmanzhang/resource_machine_learning/blob/master/deep_learning/gru.ipynb">例子</a>中的 <strong>手写GRU</strong> 的 <code>GRUCell</code> 代码片段。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GRUCell</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(GRUCell, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.hidden_size = hidden_size</span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.update_gate = nn.Linear(input_size + hidden_size, hidden_size)</span><br><span class="line">        <span class="variable language_">self</span>.reset_gate = nn.Linear(input_size + hidden_size, hidden_size)</span><br><span class="line">        <span class="variable language_">self</span>.hidden_candidate = nn.Linear(input_size + hidden_size, hidden_size)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, hidden</span>):</span><br><span class="line">        <span class="comment"># print(f&quot;cell forward : &#123;x.shape&#125; &#123;hidden.shape&#125;&quot;)</span></span><br><span class="line">        <span class="comment"># cell forward : torch.Size([1]) torch.Size([50])</span></span><br><span class="line">        combined = torch.cat((x, hidden), dim=<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        update_gate = torch.sigmoid(<span class="variable language_">self</span>.update_gate(combined))</span><br><span class="line">        reset_gate = torch.sigmoid(<span class="variable language_">self</span>.reset_gate(combined))</span><br><span class="line">        <span class="comment"># process reset</span></span><br><span class="line">        combined_hidden = torch.cat((x, reset_gate * hidden), dim=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># activate after process reset</span></span><br><span class="line">        hidden_candidate = torch.tanh(<span class="variable language_">self</span>.hidden_candidate(combined_hidden))</span><br><span class="line">        <span class="comment"># process update</span></span><br><span class="line">        hidden = (<span class="number">1</span> - update_gate) * hidden + update_gate * hidden_candidate</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> hidden</span><br></pre></td></tr></table></figure>
<p>请结合注释和原理图，进行阅读。</p>
<p>与 <code>LSTM</code> 对比   </p>
<ol>
<li><strong>没有</strong>了 <code>Cell</code> 的记忆，他是直接修改了 隐状态 <code>hidden</code></li>
<li>只有两个门，更新门 和 重置门，结构更加的简单</li>
</ol>
<p><em>总结</em>   </p>
<ol>
<li>上述逻辑 都是简单的 <strong>全链接</strong> 和 <strong>激活函数</strong> 构成，所以理解原理是最重要的，内部都是组装而已</li>
<li>因为是手写 <code>GRUCell</code>，简化了逻辑，仅仅是针对 单个数据 + 记忆 处理；sequence 的数据 就是在训练的时候，进行循环；batch 的概念，当然也是没有的。<blockquote>
<p>⚠️ 这里影响到了 1. X, y 的数据结构； 2. 模型内的 forward 的处理方法；3. train 的方法。可以对比文件后面的  nn.GRU 的解决方法，来一起服用。<br>最好的方式当然是把 手写 GRUCell 改写成 (batch, sequence, input_feature) X 数据结构 都适用的代码。</p>
</blockquote>
</li>
</ol>
<h3 id="例子-1"><a href="#例子-1" class="headerlink" title="例子"></a>例子</h3><p>与 <code>LSTM</code> 的例子的题目是一致的，分析的过程也是一致。</p>
<p>详细的代码可以直接访问 ipynb 文件<a href="https://gitee.com/oldmanzhang/resource_machine_learning/blob/master/deep_learning/gru.ipynb">链接</a>。里面有 手动GRU 和 nn.GRU 两种不同的实现方案。</p>
<h2 id="-1"><a href="#-1" class="headerlink" title=""></a></h2><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol>
<li>无论是 <code>RNN</code> 和 <code>类RNN</code> 模型 <code>LSTM</code>&#x2F;<code>GRU</code> 都是 <strong>输出时间步的隐状态</strong>，这是 时序序列相关的模型的关键</li>
<li><code>LSTM</code> 和 <code>GRU</code> ，具有模型的可解释性，同时他是解决其他问题的方式用在定义模型上，体现跨学科学习的重要性</li>
<li><code>LSTM</code> 和 <code>GRU</code> 在使用场景上都可以交替使用，看哪个比较适合</li>
</ol>
]]></content>
      <categories>
        <category>学肥AI</category>
      </categories>
      <tags>
        <tag>学肥AI</tag>
        <tag>深度学习</tag>
        <tag>LSTM</tag>
        <tag>GRU</tag>
      </tags>
  </entry>
  <entry>
    <title>微信云开发及云函数</title>
    <url>/2024/11/07/%5B%E5%AD%A6%E8%82%A5%E5%BE%AE%E4%BF%A1%E5%BC%80%E5%8F%91%5D%20%E5%BE%AE%E4%BF%A1%E4%BA%91%E5%BC%80%E5%8F%91-%E4%BA%91%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<h2 id="云开发是什么"><a href="#云开发是什么" class="headerlink" title="云开发是什么"></a>云开发是什么</h2><p>如果微信小程序是前端，那么后端就是 服务器&#x2F;后端代码运行&#x2F;云存储&#x2F;数据库等，前后端配合才是一个完整的面向服务的程序。</p>
<p>前后端配置，上述是正常的开发的组合。但是后端涉及到很多繁琐的步骤，比如 域名申请&#x2F;域名解析&#x2F;后端代码部署…一系列的问题。等解决好这些问题，时间都耗费了半天了。</p>
<p>微信云开发，就是为了解决上述的问题。在 <strong>微信开发者工具</strong> 中就可以一键开发 <strong>数据库&#x2F;云函数&#x2F;云存储</strong>，方便灵活。</p>
<p>相应的网站说明： <a href="https://developers.weixin.qq.com/miniprogram/dev/wxcloudservice/wxcloud/guide/init.html">云开发 </a>。下面就着 云函数，说明一下。</p>
<h2 id="云函数-hello-world"><a href="#云函数-hello-world" class="headerlink" title="云函数 hello world"></a>云函数 hello world</h2><ol>
<li>新建一个 <strong>云环境</strong>，打开 _微信开发者工具_，点击 <em>云开发</em> 按钮。</li>
</ol>
<p><img src="https://qiniu.oldzhangtech.com/mdpic/a3303d65-ae28-4a4a-b024-d00ad6ff4095_ff179b1c-7af2-4c3e-90a7-90529e880779.png"></p>
<p><img src="https://qiniu.oldzhangtech.com/mdpic/55118c6e-7dd0-4378-aaad-398c8fa4f6f9_d3cdcadd-4568-4b51-81ad-2d66fe249616.png"></p>
<p>红色箭头是 <code>envId</code>，后续在代码中用上。暂时先建立一个环境 <code>test</code>。</p>
<blockquote>
<p>❓环境是什么？</p>
<p>就是用来隔离不同 云函数&#x2F;数据库&#x2F;云存储的 空间。可以想象，不同的环境，就等于不同的房子；不同的房子里面虽然都有 桌子椅子，但是他们都是独立的。</p>
</blockquote>
<ol start="2">
<li>新建目录，<code>cloudfunctions</code> ，是云函数的保存目录。在项目根目录找到 <code>project.config.json</code> 文件，新增 <code>cloudfunctionRoot</code> 字段，<code>value</code> 就是 <code>cloudfunctions</code>。</li>
</ol>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">   <span class="string">&quot;cloudfunctionRoot&quot;</span>: <span class="string">&quot;cloudfunctions/&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在 <strong>新建的目录</strong> 上要选择你要在 <code>哪个环境</code> 中进行开发云函数，比如下图，选择了 新建的 <code>test</code> 环境。</p>
<p><img src="https://qiniu.oldzhangtech.com/mdpic/5bed85c0-d45a-4bc6-84b7-4b3601f05b1d_678f3efa-eef6-4d5d-94a6-f9f2acf75f09.png"></p>
<ol start="3">
<li>创建云函数，在 <code>cloudfunctions</code> 目录 右键，点击 【新建 node.js 云函数】，名字叫 <code>add</code>。</li>
</ol>
<p><img src="https://qiniu.oldzhangtech.com/mdpic/1e29e888-7ffc-4c8e-ad45-9211e11c3688_6c840fc5-13f7-4e5e-b1c0-257e5525e4c3.png"></p>
<p>add 目录下 增加多 3 个文件</p>
<p><img src="https://qiniu.oldzhangtech.com/mdpic/b062fd6a-be83-4b1f-adaa-abc4b1c445aa_55aae544-8c59-4f95-8fee-fa139fc5b325.png"></p>
<ol start="4">
<li>修改 <code>add/index.js</code> ，全文替换成下面的代码。</li>
</ol>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// 云函数入口函数</span></span><br><span class="line"><span class="built_in">exports</span>.<span class="property">main</span> = <span class="title function_">async</span> (event, context) =&gt; &#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  <span class="keyword">return</span> &#123;</span><br><span class="line">    <span class="attr">sum</span>: event.<span class="property">a</span> + event.<span class="property">b</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<ol start="5">
<li>修改 <code>package.json</code>， 删除 <code>dependencies</code> 里面的值，类似的代码如下</li>
</ol>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;dependencies&quot;</span>: &#123;</span><br><span class="line">    </span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>



<ol start="6">
<li>上传函数，变成 <code>云</code></li>
</ol>
<p><img src="https://qiniu.oldzhangtech.com/mdpic/8677fd70-f4e7-48dc-8151-500ce61fc6c5_2e030105-4f2e-4076-9e42-aa5d749a7d49.png"></p>
<blockquote>
<p>如果，有以下的情况</p>
<ol>
<li><code>index.js</code> 中 <strong>没有</strong> <code>const cloud = require(&#39;wx-server-sdk&#39;)</code> 代码</li>
<li><code>package.json</code> 的 <code>dependencies</code> 字段 没有值</li>
</ol>
<p>就可以  点击【上传并部署: 所有文件】菜单，比如 本例。</p>
<p>如果有上述的情况，就可以  点击【上传并部署: 云端安装依赖】菜单。</p>
<p><img src="https://qiniu.oldzhangtech.com/mdpic/beccdf18-d285-4e40-a6f3-1ab02736ce83_c349e39c-ec34-4d3a-9292-92cbc54a50e9.png"></p>
</blockquote>
<blockquote>
<p>💡 ‘wx-server-sdk’ 的安装，是为了 云函数中可以 调用 <strong>数据库</strong> 和 <strong>云存储</strong>。如果你在微信小程序中是直接 调用 数据库 和 云存储，就没有必要在 云函数 中倒一倒了。</p>
</blockquote>
<ol start="7">
<li>调试云函数，在 <strong>开发者工具</strong> 就可以调试刚刚上传的云函数</li>
</ol>
<p><img src="https://qiniu.oldzhangtech.com/mdpic/1ae346b2-e450-4d10-b42a-c05843452c69_385b46fd-7871-4826-8fb7-348178fcd442.png"></p>
<ol start="8">
<li>小程序中调用 云函数 add</li>
</ol>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">wx.<span class="property">cloud</span>.<span class="title function_">init</span>(&#123;</span><br><span class="line">      <span class="attr">env</span>: <span class="string">&#x27;test-safjlfjasdfasf&#x27;</span></span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">wx.<span class="property">cloud</span>.<span class="title function_">callFunction</span>(&#123;</span><br><span class="line">      <span class="comment">// 云函数名称</span></span><br><span class="line">      <span class="attr">name</span>: <span class="string">&#x27;add&#x27;</span>,</span><br><span class="line">      <span class="comment">// 传给云函数的参数</span></span><br><span class="line">      <span class="attr">data</span>: &#123;</span><br><span class="line">        <span class="attr">a</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="attr">b</span>: <span class="number">2</span>,</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">success</span>: <span class="keyword">function</span>(<span class="params">res</span>) &#123;</span><br><span class="line">        <span class="variable language_">console</span>.<span class="title function_">log</span>(res.<span class="property">result</span>.<span class="property">sum</span>) <span class="comment">// 3</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">fail</span>: <span class="variable language_">console</span>.<span class="property">error</span></span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure>



<ol start="9">
<li>撒花</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><em>微信云开发</em> 开发 _云函数_，可以在一个工具里面同时开发前端和后端的代码，减少繁琐的步骤，加快开发的效率。</p>
]]></content>
      <categories>
        <category>微信云开发</category>
      </categories>
      <tags>
        <tag>微信</tag>
        <tag>微信云开发</tag>
        <tag>云开发</tag>
        <tag>云函数</tag>
        <tag>云数据库</tag>
        <tag>云存储</tag>
      </tags>
  </entry>
  <entry>
    <title>CNN 的理解</title>
    <url>/2024/07/26/%5B%E5%AD%A6%E8%82%A5AI%5D%20CNN%E7%9A%84%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<h2 id="背景问题"><a href="#背景问题" class="headerlink" title="背景问题"></a>背景问题</h2><p>有时候，我们会使用深度神经网络来学习数据。如果效果不理想，常常都会 <strong>加深和加宽全连接层</strong> 来让效果更加的完善。但是加深加宽网络会引来更加多的问题，比如 梯度消失&#x2F;梯度爆炸 等。<br>特别是面对图像识别问题的时候，比如手写数字识别，一幅图片本来就有大量的信息，<code>width</code>&#x2F;<code>height</code>&#x2F;<code>3 color channels </code>等，加深加宽网络下的模型，结果往往未如理想。<br><code>CNN</code> 就是为了解决上述的问题的。<code>CNN</code> 即是 <code>Convolution Neural Network</code>，卷积神经网络。</p>
<blockquote>
<p>先讲解不同的子组件，再组合起来，形成手写的模型，这样就更加容易的去理解模型。</p>
</blockquote>
<h2 id="卷积和卷积核"><a href="#卷积和卷积核" class="headerlink" title="卷积和卷积核"></a>卷积和卷积核</h2><p><em>卷积</em><br>卷积，是 <em>来源矩阵</em> 和 <em>卷积核矩阵</em> 一个个对应位置进行相乘后，再相加的运算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对 【中心点】进行卷积运算</span></span><br><span class="line"><span class="comment"># 来源举证</span></span><br><span class="line">input_array = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">                        [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"><span class="comment"># 卷积核</span></span><br><span class="line">kernel_array = np.array([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                         [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                         [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line"><span class="comment"># 卷积操作</span></span><br><span class="line"><span class="comment"># 1*1+2*0+3*0+4*0+5*1+6*0+7*0+8*0+9*1 = 15</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对应卷积后</span></span><br><span class="line">Output:</span><br><span class="line">[[<span class="number">15</span>]]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>如果加上 <code>padding</code> 和 <code>stride</code> 的话，即卷积核就可以沿着 <em>来源矩阵</em> 滑动，完成整个卷积操作。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># padding = 1 ，stride = 1</span></span><br><span class="line"><span class="comment"># 来源矩阵</span></span><br><span class="line">input_array = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">                        [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"><span class="comment"># 卷积核</span></span><br><span class="line">kernel_array = np.array([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                         [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                         [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对应卷积后</span></span><br><span class="line">Output:</span><br><span class="line">[[ <span class="number">6</span>  <span class="number">8</span>  <span class="number">3</span>]</span><br><span class="line"> [<span class="number">12</span> <span class="number">15</span>  <span class="number">8</span>]</span><br><span class="line"> [ <span class="number">7</span> <span class="number">12</span> <span class="number">14</span>]]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>卷积核 <code>kernel&#39;s</code> <code>size</code>,<code>padding</code> , <code>stride</code> 与 输入矩阵 <code>input</code> 和输出矩阵 <code>output</code> 大小的对应关系：<br><img src="https://qiniu.oldzhangtech.com/mdpic/13a44c6b-038c-4851-acf6-e9310d02fc03_76d71cf8-f27f-4cb3-87cb-ae5cf16fe43f.png" alt="image.png"></p>
<blockquote>
<p>如果是 3*3 的卷积核，padding&#x3D;1 stride&#x3D;1，输入和输出的矩阵大小一致。</p>
</blockquote>
<p><em>卷积核</em><br>上例中，卷积核，就是 <code>kernel_array</code>，<code>[[1, 0, 0],[0, 1, 0],[0, 0, 1]]</code>，他是 3X3 的卷积核。</p>
<p>从 <a href="https://setosa.io/ev/image-kernels/">网站</a> 知道，不同的卷积核应用到相同的图片上，可以得到不同的结果。比如有一些是可以提取边缘特征，有些是可以模糊的。</p>
<p>💊 卷积核，就是提取特征的矩阵。有多少个卷积核，就是代表提取多少个特征。不同的卷积核可以提取不同的特征。</p>
<p>下面的代码，就是使用预设的卷积核（提取边缘轮廓特征的卷积核）去获取特征，我们可以看到直接输出的结果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a sample input image (1, 1, 5, 5)</span></span><br><span class="line">input_image = torch.tensor([[[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">                              [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">                              [<span class="number">3</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">                              [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">                              [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>]]]], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define a convolutional layer</span></span><br><span class="line">conv_layer = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">1</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the kernel weights for demonstration purposes (e.g., edge detection kernel)</span></span><br><span class="line">edge_detection_kernel = torch.tensor([[[[-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>],</span><br><span class="line">                                        [-<span class="number">1</span>,  <span class="number">8</span>, -<span class="number">1</span>],</span><br><span class="line">                                        [-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>]]]], dtype=torch.float32)</span><br><span class="line">conv_layer.weight = torch.nn.Parameter(edge_detection_kernel)</span><br><span class="line">conv_layer.bias = torch.nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply the convolutional layer to the input image</span></span><br><span class="line">output_image = conv_layer(input_image)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert the output to a NumPy array for visualization</span></span><br><span class="line">output_image_np = output_image.detach().numpy().squeeze()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the input and output images</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_images</span>(<span class="params">input_img, output_img</span>):</span><br><span class="line">    fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line">    axes[<span class="number">0</span>].imshow(input_img.squeeze(), cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    axes[<span class="number">0</span>].set_title(<span class="string">&#x27;Input Image&#x27;</span>)</span><br><span class="line">    axes[<span class="number">0</span>].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    axes[<span class="number">1</span>].imshow(output_img, cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    axes[<span class="number">1</span>].set_title(<span class="string">&#x27;Output Image&#x27;</span>)</span><br><span class="line">    axes[<span class="number">1</span>].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display the images</span></span><br><span class="line">visualize_images(input_image, output_image_np)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the input and output values for comparison</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Input Image:\n&quot;</span>, input_image.squeeze().numpy())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nOutput Image:\n&quot;</span>, output_image_np)</span><br></pre></td></tr></table></figure>
<p><img src="https://qiniu.oldzhangtech.com/mdpic/3423f3d3-a187-45cf-9aef-e5867ace1d1f_09bf874f-a2b9-4cc2-9c3a-287e72917b67.png" alt="image.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Input Image:</span><br><span class="line"> [[<span class="number">1.</span> <span class="number">2.</span> <span class="number">3.</span> <span class="number">0.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span> <span class="number">2.</span> <span class="number">3.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">3.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">2.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">2.</span> <span class="number">3.</span> <span class="number">0.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span> <span class="number">2.</span> <span class="number">3.</span> <span class="number">0.</span>]]</span><br><span class="line"></span><br><span class="line">Output Image:</span><br><span class="line"> [[  <span class="number">5.</span>   <span class="number">9.</span>  <span class="number">16.</span> -<span class="number">10.</span>   <span class="number">4.</span>]</span><br><span class="line"> [ -<span class="number">7.</span>  -<span class="number">4.</span>   <span class="number">4.</span>  <span class="number">14.</span>   <span class="number">2.</span>]</span><br><span class="line"> [ <span class="number">20.</span> -<span class="number">13.</span>  -<span class="number">5.</span>   <span class="number">5.</span>  -<span class="number">7.</span>]</span><br><span class="line"> [  <span class="number">2.</span>   <span class="number">5.</span>  <span class="number">13.</span> -<span class="number">12.</span>   <span class="number">3.</span>]</span><br><span class="line"> [ -<span class="number">4.</span>   <span class="number">0.</span>   <span class="number">7.</span>  <span class="number">18.</span>  -<span class="number">4.</span>]]</span><br></pre></td></tr></table></figure>
<p>从结果知道（从灰度图看不出问题，但是从数字矩阵可以看出），这个卷积核令到 黑的更加的黑， 白的更加的白，突出了特征。</p>
<p><em>卷积的好处</em></p>
<ul>
<li>共享参数</li>
</ul>
<p>卷积核，就是滑过数据的共享参数，那么他能够提取对应的特征数据。所以一个卷积核对应着一个特征。需要提取的特征，不论是在图片的哪个位置，都可以提取出来。</p>
<ul>
<li>参数减少</li>
</ul>
<p>比如 100 * 100 的图像，要去识别苹果，但是这个苹果有可能出现在上下左右，随机的位置。<br>如果使用 <code>NN</code> 神经网络 就要使用 (100 * 100, n, n) 网络， 网络的输入是每个坐标像素。但是苹果的出现的位置是随机的，即坐标是随机的，所以对应的神经元也是需要学习，导致非常难收敛。同时，参数量就是巨大的，达到 10000 * n * n。<br>但是使用 <code>CNN</code>，可以针对 苹果轮廓 训练 对应卷积核，达到识别的效果，就大大减少参数量。参数量就是 卷积核上的数值 和 后续的高维度的全链接网络的参数。</p>
<ul>
<li>位置信息的共享</li>
</ul>
<p>如果上例子中，苹果轮廓信息是共享的。</p>
<h2 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h2><p>有平均池化和最大值池化，他 <strong>没有参数学习</strong>，完全是按照默认的逻辑去运行的。</p>
<ul>
<li>平均池化，取平均值，会模糊特征，他可以考虑到所有像素的关系；</li>
<li>最大值池化，取最大值，就突出了特征了，完全是忽视了最小值。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Input_array:</span><br><span class="line">[[ <span class="number">1.</span>  <span class="number">2.</span>  <span class="number">3.</span>  <span class="number">4.</span>]</span><br><span class="line"> [ <span class="number">5.</span>  <span class="number">6.</span>  <span class="number">7.</span>  <span class="number">8.</span>]</span><br><span class="line"> [ <span class="number">9.</span> <span class="number">10.</span> <span class="number">11.</span> <span class="number">12.</span>]</span><br><span class="line"> [<span class="number">13.</span> <span class="number">14.</span> <span class="number">15.</span> <span class="number">16.</span>]]</span><br><span class="line"></span><br><span class="line">应用 kernel_size=<span class="number">2</span>, stride=<span class="number">2</span> 的 池化</span><br><span class="line"></span><br><span class="line">Average Pooled Image:</span><br><span class="line"> [[ <span class="number">3.5</span>  <span class="number">5.5</span>]</span><br><span class="line"> [<span class="number">11.5</span> <span class="number">13.5</span>]]</span><br><span class="line"></span><br><span class="line">Max Pooled Image:</span><br><span class="line"> [[ <span class="number">6.</span>  <span class="number">8.</span>]</span><br><span class="line"> [<span class="number">14.</span> <span class="number">16.</span>]]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>从结果知道，如果池化后，计算量是加上了 3&#x2F;4，同时可以保留原来图像的特征。</p>
<p><em>池化好处</em><br>就是为了减少运算量，而且没有参数，不用学习。换一句话，有很多的数据是没有意义的，计算他们投入产出比太低，不如直接放弃。</p>
<h2 id="手写简单-CNN-网络"><a href="#手写简单-CNN-网络" class="headerlink" title="手写简单 CNN 网络"></a>手写简单 CNN 网络</h2><p>组装 <em>卷积</em> 和 _池化 _的一个网络，就是一个简单的 <code>CNN</code> 网络，比如 以下的代码就是一个简单的 卷积和池化操作相结合。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 部分代码的演示</span></span><br><span class="line"><span class="comment"># conv 定义卷积</span></span><br><span class="line">conv = .Conv2d(<span class="number">1</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span></span><br><span class="line"><span class="comment"># pool 定义池化</span></span><br><span class="line">pool = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结合激活函数</span></span><br><span class="line">x = pool(relu(conv(x)))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p>所有模型（包括 <code>NN</code>，<code>CNN</code>，<code>AlexNet</code>，<code>ResNet</code>，<code>RNN</code> 等 ），都是一个个的子组件的组装，就好像积木一样。所以理解好不同的 子组件的原理和用法，就更加容易理解由他们组装而成的大模型。</p>
</blockquote>
<h3 id="完整代码检测手写数字的图片"><a href="#完整代码检测手写数字的图片" class="headerlink" title="完整代码检测手写数字的图片"></a>完整代码检测手写数字的图片</h3><p>导入手写数字的图片库，同时使用一个 简单的 CNN 来训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the CNN model</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleCNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.pool = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">64</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.pool(F.relu(<span class="variable language_">self</span>.conv1(x)))</span><br><span class="line">        x = <span class="variable language_">self</span>.pool(F.relu(<span class="variable language_">self</span>.conv2(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>)</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x))</span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load and preprocess the MNIST dataset</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">trainset = torchvision.datasets.MNIST(root=<span class="string">&#x27;../data/mnist&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">trainloader = DataLoader(trainset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">testset = torchvision.datasets.MNIST(root=<span class="string">&#x27;../data/mnist&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">testloader = DataLoader(testset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Instantiate the model, define the loss function and the optimizer</span></span><br><span class="line">model = SimpleCNN()</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training loop</span></span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(trainloader, <span class="number">0</span>):</span><br><span class="line">        inputs, labels = data</span><br><span class="line">        </span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;[Epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, Batch <span class="subst">&#123;i + <span class="number">1</span>&#125;</span>] loss: <span class="subst">&#123;running_loss / <span class="number">100</span>:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test the model</span></span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Accuracy of the network on the 10000 test images: <span class="subst">&#123;<span class="number">100</span> * correct / total:<span class="number">.2</span>f&#125;</span>%&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"># 部分打印</span></span><br><span class="line"><span class="string">[Epoch 5, Batch 600] loss: 0.075</span></span><br><span class="line"><span class="string">[Epoch 5, Batch 700] loss: 0.093</span></span><br><span class="line"><span class="string">[Epoch 5, Batch 800] loss: 0.074</span></span><br><span class="line"><span class="string">[Epoch 5, Batch 900] loss: 0.081</span></span><br><span class="line"><span class="string">Finished Training</span></span><br><span class="line"><span class="string">Accuracy of the network on the 10000 test images: 98.08%</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>模型训练的结果是另人满意的。</p>
<h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><p>上面的例子，已经说明了 CNN 的整体的框架。</p>
<p><em>结构图</em><br><img src="https://qiniu.oldzhangtech.com/mdpic/9a6823f2-13ed-4c75-ad84-17d4c79c27b6_1666cf21-784f-4e6f-8991-32451823b357.png" alt="image.png"><br>面对 分类任务，主体上分成两层： Feature Extraction 特征提取 和 Classification 分类<br><code>Feature Extraction</code>： input 就是输入的数据，Convolution 就是卷积，Pooling 就是池化，到这步为止，就是从低维度特征的提取。<br><code>Classification</code>：之后对提取后的特征进行 高维全网络学习 和 softmax 进行分类。</p>
<p><em>一句话原理</em></p>
<ol>
<li><strong>全数据</strong>先用<strong>卷积</strong>提取<strong>特征，再使用 池化层 进行数据量的简化，同时保留特征</strong></li>
<li><strong>特征</strong>再用<strong>全链接</strong>学习规律<blockquote>
<p>总结成一句话就是，每个组件都在自己擅长东西。</p>
</blockquote>
</li>
</ol>
<p>❓卷积核的 <code>out_channel</code> 是否重要？</p>
<blockquote>
<p><code>nn.Conv2d(32, 64, kernel_size=3, padding=1)</code> 中的 <code>64</code> 就是 <code>out_channel</code></p>
</blockquote>
<p>重要，他是决定了多少个卷积核，即多少个 features 可以给提取。</p>
<p>题外话，卷积核的 <code>size</code>，是决定了他可以识别的范围（接收野 receptive field），比如 小的卷积核，就可以识别精细的特征；大的，可以识别轮廓的特征。</p>
<p>❓卷积核的参数是学习而来，还是预设定的呢？<br>可以预设，也可以学习而来的。<br>但是在 CNN 中，尽量<strong>不用</strong>指定卷积核，让他去进行训练习得。 </p>
<h2 id="简要总结"><a href="#简要总结" class="headerlink" title="简要总结"></a>简要总结</h2><p><em>卷积</em></p>
<ol>
<li>共享参数</li>
<li>参数减少</li>
<li>可以有位置信息的共享</li>
</ol>
<p><em>卷积核</em></p>
<ol>
<li>out_channel 决定了提取多少个特征</li>
<li>kernel_size 决定你有多少的视野</li>
</ol>
<p>_池化_，就是为了简化数量的同时，又可以保留特征；_全链接层_，对抽取的特征进行学习。</p>
<p>_CNN _就是组合上述组件，形成一个大的网络。</p>
<h2 id="优缺点和场景"><a href="#优缺点和场景" class="headerlink" title="优缺点和场景"></a>优缺点和场景</h2><ol>
<li>有空间位置信息，就可以使用卷积神经网络</li>
<li>特别图像识别</li>
</ol>
]]></content>
      <categories>
        <category>学肥AI</category>
      </categories>
      <tags>
        <tag>学肥AI</tag>
        <tag>深度学习</tag>
        <tag>CNN</tag>
        <tag>NN</tag>
        <tag>卷积神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>CNN 一些例子-其中包括 LeNet</title>
    <url>/2024/08/02/%5B%E5%AD%A6%E8%82%A5AI%5D%20CNN%20%E4%B8%80%E4%BA%9B%E4%BE%8B%E5%AD%90-%E5%85%B6%E4%B8%AD%E5%8C%85%E6%8B%AC%20LeNet/</url>
    <content><![CDATA[<h2 id="背景问题"><a href="#背景问题" class="headerlink" title="背景问题"></a>背景问题</h2><p>前文中已经对 CNN 组件 以及整体的数据流动有了解，有需要的可以翻看之前的<a href="https://blog.csdn.net/weixin_49113487/article/details/140717493?spm=1001.2014.3001.5501">文章</a>，后面我们直接应用 CNN 来解决实际问题，动手操作才是学习的根本。</p>
<p>本篇的所有的例子，详细的 <code>ipynb</code> 都可以从下面链接中找到。 NNvsCNN <a href="https://gitee.com/oldmanzhang/resource_machine_learning/blob/master/deep_learning/cnn_compareNN.ipynb">ipynb 链接</a>，识别图形 <a href="https://gitee.com/oldmanzhang/resource_machine_learning/blob/master/deep_learning/cnn_classify_triangleCycleRectangle.ipynb">ipynb 链接</a>。</p>
<h2 id="NN-vs-CNN"><a href="#NN-vs-CNN" class="headerlink" title="NN vs CNN"></a>NN vs CNN</h2><p>本例子的目的是：同样面对 <code>mnist</code> 手写数字数据集的时候，对比 使用 <code>NN</code> 和 使用 <code>CNN</code> ，看一下他们表现的差异。可运行文件 <code>ipynb</code> 的<a href="https://gitee.com/oldmanzhang/resource_machine_learning/blob/master/deep_learning/cnn_compareNN.ipynb">链接</a>。</p>
<h3 id="prepare-data"><a href="#prepare-data" class="headerlink" title="prepare data"></a>prepare data</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"><span class="comment"># %%</span></span><br><span class="line">transformation = torchvision.transforms.ToTensor()</span><br><span class="line"><span class="comment"># 执行调整文件的位置</span></span><br><span class="line">train_dataset = torchvision.datasets.MNIST(root=<span class="string">&#x27;../data/mnist/&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transformation)</span><br><span class="line">test_dataset = torchvision.datasets.MNIST(root=<span class="string">&#x27;../data/mnist/&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transformation)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h3 id="NN-的实现"><a href="#NN-的实现" class="headerlink" title="NN 的实现"></a>NN 的实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> * <span class="comment"># tqdm用于显示进度条并评估任务时间开销</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_sizes, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.fcin = nn.Linear(input_size, hidden_sizes[<span class="number">0</span>]) <span class="keyword">if</span> hidden_sizes <span class="keyword">else</span> nn.Linear(input_size, output_size)</span><br><span class="line">        <span class="variable language_">self</span>.fcout = nn.Linear(hidden_sizes[-<span class="number">1</span>], output_size) <span class="keyword">if</span> hidden_sizes <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Create a list of intermediate layers</span></span><br><span class="line">        layers = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(hidden_sizes) - <span class="number">1</span>):</span><br><span class="line">            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+<span class="number">1</span>]))</span><br><span class="line">            layers.append(nn.ReLU())</span><br><span class="line">        <span class="comment"># Convert the list of layers into a Sequential module</span></span><br><span class="line">        <span class="variable language_">self</span>.hidden_layers = nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">     </span><br><span class="line">        out = <span class="variable language_">self</span>.fcin(x)</span><br><span class="line">        out = <span class="variable language_">self</span>.relu(out)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Pass through the hidden layers</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.hidden_layers) &gt; <span class="number">0</span>:</span><br><span class="line">            out = <span class="variable language_">self</span>.hidden_layers(out)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将上一步结果传递给fcout</span></span><br><span class="line">        out = <span class="variable language_">self</span>.fcout(out)</span><br><span class="line">        <span class="comment"># 返回结果</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># %%</span></span><br><span class="line">input_size = <span class="number">28</span>*<span class="number">28</span></span><br><span class="line">output_size = <span class="number">10</span></span><br><span class="line">model = NNet(input_size, [<span class="number">512</span>, <span class="number">512</span>], output_size)</span><br><span class="line"><span class="built_in">print</span>(summary(model))</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">model, data_loader</span>):</span><br><span class="line"><span class="comment">#     评估</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> data_loader:</span><br><span class="line">            x = x.view(-<span class="number">1</span>, input_size)</span><br><span class="line">            logits = model(x)</span><br><span class="line"><span class="comment">#             _ 是 value ， predicted 是 index</span></span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(logits.data, <span class="number">1</span>)</span><br><span class="line">            total += y.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == y).<span class="built_in">sum</span>().item()</span><br><span class="line">    <span class="keyword">return</span> correct / total</span><br><span class="line"></span><br><span class="line">loss_history = []  <span class="comment"># 创建损失历史记录列表</span></span><br><span class="line">acc_history = []   <span class="comment"># 创建准确率历史记录列表</span></span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(num_epochs), file=sys.stdout):</span><br><span class="line">    <span class="comment"># 记录损失和预测正确数</span></span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    total_correct = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        <span class="comment"># 将图像和标签转换成张量</span></span><br><span class="line">        <span class="comment"># [64, 1, 28, 28] -&gt; [64, 784]</span></span><br><span class="line">        images = images.view(-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>)  </span><br><span class="line">        labels = labels.long()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        outputs = model(images)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 记录训练集loss</span></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播和优化</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    loss_history.append(np.log10(total_loss))    </span><br><span class="line">    accuracy = evaluate(model, test_dataloader)</span><br><span class="line">    acc_history.append(accuracy)</span><br><span class="line">    <span class="keyword">if</span>(epoch%<span class="number">3</span> == <span class="number">0</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>: test accuracy = <span class="subst">&#123;acc_history[-<span class="number">1</span>]:<span class="number">.2</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Matplotlib绘制损失和准确率的曲线图</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.plot(loss_history, label=<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">plt.plot(acc_history, label=<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出准确率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy:&quot;</span>, acc_history[-<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p><img src="https://qiniu.oldzhangtech.com/mdpic/55ff4334-4439-494f-aa82-709af8e79db0_d98edf1a-29fd-456c-8a2e-8e4b28464ff6.png" alt="image.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 部分的输出</span></span><br><span class="line">Accuracy: <span class="number">0.9422</span></span><br></pre></td></tr></table></figure>
<h3 id="CNN-的实现"><a href="#CNN-的实现" class="headerlink" title="CNN 的实现"></a>CNN 的实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入必要的库，torchinfo用于查看模型结构</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义LeNet的网络结构</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleCnnNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">10</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleCnnNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line"><span class="comment">#         步长默认为1，填充默认为0 一定要记得</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 卷积层2：输入6个通道，输出16个通道，卷积核大小为5x5</span></span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(in_channels=<span class="number">6</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 全连接层1：输入16x4x4=256个节点，输出120个节点</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(in_features=<span class="number">16</span> * <span class="number">4</span> * <span class="number">4</span>, out_features=<span class="number">120</span>)</span><br><span class="line">        <span class="comment"># 全连接层2：输入120个节点，输出84个节点</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(in_features=<span class="number">120</span>, out_features=<span class="number">84</span>)</span><br><span class="line">        <span class="comment"># 输出层：输入84个节点，输出10个节点</span></span><br><span class="line">        <span class="variable language_">self</span>.fc3 = nn.Linear(in_features=<span class="number">84</span>, out_features=num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 使用ReLU激活函数，并进行最大池化</span></span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.conv1(x)) </span><br><span class="line">        <span class="comment"># input: 1,28,28, output: 6,24,24</span></span><br><span class="line">        <span class="comment"># output 计算逻辑：(28-5+2*0)/1 + 1 = 24</span></span><br><span class="line">        x = nn.functional.max_pool2d(x, kernel_size=<span class="number">2</span>)  <span class="comment"># output: 6,12,12</span></span><br><span class="line">        <span class="comment"># 使用ReLU激活函数，并进行最大池化</span></span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.conv2(x))  </span><br><span class="line">        <span class="comment"># input: 6,12,12, output: 16,8,8</span></span><br><span class="line">        <span class="comment"># output 计算逻辑：(12-5+2*0)/1 + 1 = 8</span></span><br><span class="line">        x = nn.functional.max_pool2d(x, kernel_size=<span class="number">2</span>)  </span><br><span class="line">        <span class="comment"># output: 16,4,4</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将多维张量展平为一维张量</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">4</span> * <span class="number">4</span>)</span><br><span class="line">        <span class="comment"># 全连接层</span></span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.fc1(x))</span><br><span class="line">        <span class="comment"># 全连接层</span></span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.fc2(x))</span><br><span class="line">        <span class="comment"># 全连接层</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 查看模型结构及参数量，input_size表示示例输入数据的维度信息</span></span><br><span class="line"><span class="built_in">print</span>(summary(SimpleCnnNet(), input_size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入必要的库</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> * <span class="comment"># tqdm用于显示进度条并评估任务时间开销</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置随机种子</span></span><br><span class="line">torch.manual_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型、优化器、损失函数</span></span><br><span class="line">model = SimpleCnnNet()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置数据变换和数据加载器</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 将数据转换为张量</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置epoch数并开始训练</span></span><br><span class="line">num_epochs = <span class="number">10</span>  <span class="comment"># 设置epoch数</span></span><br><span class="line">loss_history = []  <span class="comment"># 创建损失历史记录列表</span></span><br><span class="line">acc_history = []   <span class="comment"># 创建准确率历史记录列表</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tqdm用于显示进度条并评估任务时间开销</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(num_epochs), file=sys.stdout):</span><br><span class="line">    <span class="comment"># 记录损失和预测正确数</span></span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    total_correct = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 批量训练</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> train_dataloader:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 预测、损失函数、反向传播</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 记录训练集loss</span></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 测试模型，不计算梯度</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> test_dataloader:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 预测</span></span><br><span class="line">            outputs = model(inputs)</span><br><span class="line">            <span class="comment"># 记录测试集预测正确数</span></span><br><span class="line">            total_correct += (outputs.argmax(<span class="number">1</span>) == labels).<span class="built_in">sum</span>().item()</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 记录训练集损失和测试集准确率</span></span><br><span class="line">    loss_history.append(np.log10(total_loss))  <span class="comment"># 将损失加入损失历史记录列表，由于数值有时较大，这里取对数</span></span><br><span class="line">    acc_history.append(total_correct / <span class="built_in">len</span>(test_dataset))<span class="comment"># 将准确率加入准确率历史记录列表</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 打印中间值</span></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        tqdm.write(<span class="string">&quot;Epoch: &#123;0&#125; Loss: &#123;1&#125; Acc: &#123;2&#125;&quot;</span>.<span class="built_in">format</span>(epoch, loss_history[-<span class="number">1</span>], acc_history[-<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Matplotlib绘制损失和准确率的曲线图</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.plot(loss_history, label=<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">plt.plot(acc_history, label=<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出准确率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy:&quot;</span>, acc_history[-<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<p><img src="https://qiniu.oldzhangtech.com/mdpic/4413e664-0d64-4025-8085-61dfb55dd43d_069afa8c-7ef4-416f-ac1b-9f499a31c7a5.png" alt="image.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 部分的输出</span></span><br><span class="line">Accuracy: <span class="number">0.9832</span></span><br></pre></td></tr></table></figure>
<h3 id="对比效果"><a href="#对比效果" class="headerlink" title="对比效果"></a>对比效果</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用Matplotlib绘制损失和准确率的曲线图</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.plot(nn_loss_history, label=<span class="string">&#x27;nn loss&#x27;</span>)</span><br><span class="line">plt.plot(nn_acc_history, label=<span class="string">&#x27;nn accuracy&#x27;</span>)</span><br><span class="line">plt.plot(cnn_loss_history, label=<span class="string">&#x27;cnn loss&#x27;</span>)</span><br><span class="line">plt.plot(cnn_acc_history, label=<span class="string">&#x27;cnn accuracy&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出准确率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;ACCURACY:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;nn:&quot;</span>, nn_acc_history[-<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;cnn:&quot;</span>, cnn_acc_history[-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算参数量</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_parameters</span>(<span class="params">model</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line"></span><br><span class="line">nn_total_params = count_parameters(nn_model)</span><br><span class="line">cnn_total_params = count_parameters(cnn_model)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;count of PARAMETERS:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;nn:&quot;</span>, nn_total_params)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;cnn:&quot;</span>, cnn_total_params)</span><br></pre></td></tr></table></figure>
<p><img src="https://qiniu.oldzhangtech.com/mdpic/3e483fa6-5eec-4eb5-8ce9-6cfc2967bd9b_3874b442-7387-4608-bf5a-0ff2e2b34337.png" alt="image.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ACCURACY:</span><br><span class="line">nn: <span class="number">0.9422</span></span><br><span class="line">cnn: <span class="number">0.9832</span></span><br><span class="line"></span><br><span class="line">count of PARAMETERS:</span><br><span class="line">nn: <span class="number">669706</span></span><br><span class="line">cnn: <span class="number">44426</span></span><br></pre></td></tr></table></figure>

<p>🔥 从图中可以看到 <code>CNN</code> 比 <code>NN</code> 的 <strong>准确度更加高</strong>， 同时他的使用的 <strong>参数量会更加的少</strong>。</p>
<p>💡 当然模型应用不同的超参数会有不同的效果，可以对 <code>NN</code> 和 <code>CNN</code> 进行修改，看一下效果如何。同时 <a href="https://gitee.com/oldmanzhang/resource_machine_learning/blob/master/deep_learning/cnn_compareNN.ipynb">链接</a> 中已经有 <code>girdSearch</code> 的方法，用于寻找最佳的 <code>NN</code> 参数。</p>
<h2 id="LeNet-网络-识别手写数字"><a href="#LeNet-网络-识别手写数字" class="headerlink" title="LeNet 网络 识别手写数字"></a>LeNet 网络 识别手写数字</h2><p>参考了别人的各种卷积神经网络的时间线贴图。<br><img src="https://qiniu.oldzhangtech.com/mdpic/20431588-e607-4106-817c-68d54848a902_83fc8c68-cce1-4663-85a0-e94f939f9052.png"><br><code>LeNet</code> 为卷积神经网网络奠定了基石。所有的后续的网络都是基于这个网络进行扩展的。</p>
<h3 id="整体框架"><a href="#整体框架" class="headerlink" title="整体框架"></a>整体框架</h3><p><img src="https://qiniu.oldzhangtech.com/mdpic/b990ca3c-7939-4610-b9d8-cafc9586a8bd_5119f828-a66b-4e1d-a03b-7703add2107a.png"><br><code>LeNet</code> 是由 多个卷积层，池化层，全连接层 组合构建而来。<br>结构并不复杂，下面就可以手动去实现这个网络。如果对 <code>卷积</code>，<code>池化</code> 并不熟悉的，可以前往 <a href="https://blog.csdn.net/weixin_49113487/article/details/140717493">文章</a> 进行了解。</p>
<h3 id="手写-LeNet-网络"><a href="#手写-LeNet-网络" class="headerlink" title="手写 LeNet 网络"></a>手写 LeNet 网络</h3><p>其实手写 <code>LeNet</code>  就是 上述例子中的 <code>CNN</code> 的 <code>SimpleCnnNet</code>。下面列出部分的代码片段。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleCnnNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">10</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleCnnNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line"><span class="comment">#         步长默认为1，填充默认为0 一定要记得</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 卷积层2：输入6个通道，输出16个通道，卷积核大小为5x5 </span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(in_channels=<span class="number">6</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 全连接层1：输入16x4x4=256个节点，输出120个节点</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(in_features=<span class="number">16</span> * <span class="number">4</span> * <span class="number">4</span>, out_features=<span class="number">120</span>)</span><br><span class="line">        <span class="comment"># 全连接层2：输入120个节点，输出84个节点</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(in_features=<span class="number">120</span>, out_features=<span class="number">84</span>)</span><br><span class="line">        <span class="comment"># 输出层：输入84个节点，输出10个节点</span></span><br><span class="line">        <span class="variable language_">self</span>.fc3 = nn.Linear(in_features=<span class="number">84</span>, out_features=num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 使用ReLU激活函数，并进行最大池化</span></span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.conv1(x)) </span><br><span class="line">        <span class="comment"># input: 1,28,28, output: 6,24,24</span></span><br><span class="line">        <span class="comment"># output 计算逻辑：(28-5+2*0)/1 + 1 = 24</span></span><br><span class="line">        x = nn.functional.max_pool2d(x, kernel_size=<span class="number">2</span>)  <span class="comment"># output: 6,12,12</span></span><br><span class="line">        <span class="comment"># 使用ReLU激活函数，并进行最大池化</span></span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.conv2(x))  </span><br><span class="line">        <span class="comment"># input: 6,12,12, output: 16,8,8</span></span><br><span class="line">        <span class="comment"># output 计算逻辑：(12-5+2*0)/1 + 1 = 8</span></span><br><span class="line">        x = nn.functional.max_pool2d(x, kernel_size=<span class="number">2</span>)  </span><br><span class="line">        <span class="comment"># output: 16,4,4</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将多维张量展平为一维张量</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">4</span> * <span class="number">4</span>)</span><br><span class="line">        <span class="comment"># 全连接层</span></span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.fc1(x))</span><br><span class="line">        <span class="comment"># 全连接层</span></span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.fc2(x))</span><br><span class="line">        <span class="comment"># 全连接层</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>由上面定义的模型的结构得知：<br><code>Feature extraction</code>： 由 2 *【卷积 *  激活   * 池化】组成，每层的卷积和池化的参数有不同。用于抽取特征。<br><code>Classification</code>： 由 2 *【全连接层 * 激活】 组成。对特征进行逻辑关联。</p>
<h2 id="识别三角形-圆形-正方形"><a href="#识别三角形-圆形-正方形" class="headerlink" title="识别三角形&#x2F;圆形&#x2F;正方形"></a>识别三角形&#x2F;圆形&#x2F;正方形</h2><p>目的：生成随机的 三角形&#x2F;圆形&#x2F;正方形 的黑白图数据集，并用 CNN 的模型识别他们。可运行 ipynb 的文件<a href="https://gitee.com/oldmanzhang/resource_machine_learning/blob/master/deep_learning/cnn_classify_triangleCycleRectangle.ipynb">链接</a>。<br><em>Prepare data</em></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># !pip install opencv-python</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_shape</span>(<span class="params">shape, img_size=<span class="number">1000</span></span>):</span><br><span class="line">    img = np.zeros((img_size, img_size), dtype=np.uint8)</span><br><span class="line">    <span class="keyword">if</span> shape == <span class="string">&#x27;circle&#x27;</span>:</span><br><span class="line">        radius = np.random.randint(img_size // <span class="number">8</span>, img_size // <span class="number">4</span>)</span><br><span class="line">        center = (np.random.randint(radius, img_size - radius), np.random.randint(radius, img_size - radius))</span><br><span class="line">        cv2.circle(img, center, radius, <span class="number">255</span>, -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">elif</span> shape == <span class="string">&#x27;triangle&#x27;</span>:</span><br><span class="line">        pt1 = (np.random.randint(<span class="number">0</span>, img_size), np.random.randint(<span class="number">0</span>, img_size))</span><br><span class="line">        pt2 = (np.random.randint(<span class="number">0</span>, img_size), np.random.randint(<span class="number">0</span>, img_size))</span><br><span class="line">        pt3 = (np.random.randint(<span class="number">0</span>, img_size), np.random.randint(<span class="number">0</span>, img_size))</span><br><span class="line">        points = np.array([pt1, pt2, pt3])</span><br><span class="line">        cv2.drawContours(img, [points], <span class="number">0</span>, <span class="number">255</span>, -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">elif</span> shape == <span class="string">&#x27;rectangle&#x27;</span>:</span><br><span class="line">        pt1 = (np.random.randint(<span class="number">0</span>, img_size // <span class="number">2</span>), np.random.randint(<span class="number">0</span>, img_size // <span class="number">2</span>))</span><br><span class="line">        pt2 = (np.random.randint(img_size // <span class="number">2</span>, img_size), np.random.randint(img_size // <span class="number">2</span>, img_size))</span><br><span class="line">        cv2.rectangle(img, pt1, pt2, <span class="number">255</span>, -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_dataset</span>(<span class="params">num_samples=<span class="number">1000</span>, img_size=<span class="number">1000</span></span>):</span><br><span class="line">    shapes = [<span class="string">&#x27;circle&#x27;</span>, <span class="string">&#x27;triangle&#x27;</span>, <span class="string">&#x27;rectangle&#x27;</span>]</span><br><span class="line">    data = []</span><br><span class="line">    labels = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_samples):</span><br><span class="line">        shape = np.random.choice(shapes)</span><br><span class="line">        img = create_shape(shape, img_size)</span><br><span class="line">        data.append(img)</span><br><span class="line">        labels.append(shapes.index(shape))</span><br><span class="line">    <span class="keyword">return</span> np.array(data), np.array(labels)</span><br></pre></td></tr></table></figure>

<p>可视化图像。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Visualize some samples</span></span><br><span class="line"><span class="comment"># img_size = 200 是比较好的体现出 形状</span></span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">3</span>, figsize=(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> i, shape <span class="keyword">in</span> <span class="built_in">enumerate</span>([<span class="string">&#x27;circle&#x27;</span>, <span class="string">&#x27;triangle&#x27;</span>, <span class="string">&#x27;rectangle&#x27;</span>]):</span><br><span class="line">    axes[i].imshow(create_shape(shape, img_size=<span class="number">200</span>), cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    axes[i].set_title(shape)</span><br><span class="line">    axes[i].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://qiniu.oldzhangtech.com/mdpic/4a51309b-3fe5-4280-9bbd-da97b8c6925d_bf9181d6-8b22-499c-a013-52ec993b5c47.png" alt="image.png"></p>
<p>随机检查数据集，检查数据是否合理。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num = <span class="number">5</span></span><br><span class="line">ran_indexes = np.random.randint(<span class="number">0</span>, <span class="built_in">len</span>(data), num)</span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, num, figsize=(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num):</span><br><span class="line">    axes[ i].imshow(data[ran_indexes[i]], cmap=<span class="string">&#x27;gray&#x27;</span>)  <span class="comment">## &#x27;Axes&#x27; object is not subscriptable</span></span><br><span class="line">    axes[ i].set_title(<span class="string">f&#x27;index : <span class="subst">&#123;ran_indexes[i]&#125;</span> <span class="subst">&#123;labels[ran_indexes[i]]&#125;</span>&#x27;</span>)</span><br><span class="line">    axes[ i].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 0: cycle, 1: triangle, 2: rectangle</span></span><br></pre></td></tr></table></figure>
<p><img src="https://qiniu.oldzhangtech.com/mdpic/0e26b843-b82c-4cb3-8617-54f7eb265e20_f96bca9b-901b-4724-bd06-6b9fab029a9a.png" alt="image.png"></p>
<p><em>define model</em></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ShapeDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data, labels, transform=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.data = data</span><br><span class="line">        <span class="variable language_">self</span>.labels = labels</span><br><span class="line">        <span class="variable language_">self</span>.transform = transform</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        image = <span class="variable language_">self</span>.data[idx]</span><br><span class="line">        label = <span class="variable language_">self</span>.labels[idx]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.transform:</span><br><span class="line">            image = <span class="variable language_">self</span>.transform(image)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> image, label</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleCNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># output:32*200*200</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># output:32*50*50</span></span><br><span class="line">        <span class="variable language_">self</span>.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># output:64*50*50</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># output:128</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">64</span> * <span class="number">50</span> * <span class="number">50</span>, <span class="number">128</span>)  <span class="comment"># Adjust input size for the new image size</span></span><br><span class="line">        <span class="comment"># output: 3</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">128</span>, <span class="number">3</span>)  <span class="comment"># 3 classes: circle, triangle, rectangle</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.pool(F.relu(<span class="variable language_">self</span>.conv1(x)))</span><br><span class="line">        x = <span class="variable language_">self</span>.pool(F.relu(<span class="variable language_">self</span>.conv2(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">64</span> * <span class="number">50</span> * <span class="number">50</span>)  <span class="comment"># Flatten the tensor</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x))</span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<p><em>train model</em></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Data transformation</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    <span class="comment"># transforms.Grayscale(num_output_channels=1),  # Ensure single channel for grayscale images</span></span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the dataset and data loaders</span></span><br><span class="line">train_dataset = ShapeDataset(data, labels, transform=transform)</span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Instantiate the model</span></span><br><span class="line">model = SimpleCNN()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define loss function and optimizer</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># validate dataset</span></span><br><span class="line">val_data, val_labels = generate_dataset(num_samples=<span class="number">200</span>, img_size=<span class="number">200</span>)</span><br><span class="line">val_dataset = ShapeDataset(val_data, val_labels, transform=transform)</span><br><span class="line">val_loader = DataLoader(val_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Training method</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">model, train_loader, val_loader, criterion, optimizer, num_epochs</span>):</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        running_loss = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> _data, _labels <span class="keyword">in</span> train_loader:</span><br><span class="line">            <span class="comment"># Zero the parameter gradients</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Forward pass</span></span><br><span class="line">            outputs = model(_data)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># print(_data.shape, _labels.shape, outputs.shape)</span></span><br><span class="line"></span><br><span class="line">            loss = criterion(outputs, _labels)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Backward pass and optimize</span></span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Print statistics</span></span><br><span class="line">            running_loss += loss.item()</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>], Loss: <span class="subst">&#123;running_loss / <span class="built_in">len</span>(train_loader):<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">        validate(model, val_loader, criterion)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Validation method</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">validate</span>(<span class="params">model, val_loader, criterion</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()  <span class="comment"># Set the model to evaluation mode</span></span><br><span class="line">    val_loss = <span class="number">0.0</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> _data, _labels <span class="keyword">in</span> val_loader:</span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">            <span class="comment"># Forward pass</span></span><br><span class="line">            outputs = model(_data)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># print(_data.shape, _labels.shape, outputs.shape)</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Calculate accuracy</span></span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">            total += _labels.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == _labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Optional: Calculate loss if needed</span></span><br><span class="line">            <span class="comment"># loss = criterion(outputs, _labels)</span></span><br><span class="line">            <span class="comment"># val_loss += loss.item()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Optional: Calculate average loss if needed</span></span><br><span class="line">    <span class="comment"># avg_loss = val_loss / len(val_loader)</span></span><br><span class="line">    accuracy = <span class="number">100</span> * correct / total</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Accuracy: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>%&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model with validation</span></span><br><span class="line">num_epochs = <span class="number">20</span></span><br><span class="line">train(model, train_loader, val_loader, criterion, optimizer, num_epochs)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Epoch [17/20], Loss: 0.0005</span></span><br><span class="line"><span class="string">Accuracy: 92.00%</span></span><br><span class="line"><span class="string">Epoch [18/20], Loss: 0.0005</span></span><br><span class="line"><span class="string">Accuracy: 92.00%</span></span><br><span class="line"><span class="string">Epoch [19/20], Loss: 0.0004</span></span><br><span class="line"><span class="string">Accuracy: 92.00%</span></span><br><span class="line"><span class="string">Epoch [20/20], Loss: 0.0003</span></span><br><span class="line"><span class="string">Accuracy: 92.00%</span></span><br><span class="line"><span class="string">Finished Training</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><em>evalue</em></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Generate test data</span></span><br><span class="line">test_data, test_labels = generate_dataset(num_samples=<span class="number">100</span>, img_size=<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the test dataset and data loader</span></span><br><span class="line">test_dataset = ShapeDataset(test_data, test_labels, transform=transform)</span><br><span class="line">test_loader = DataLoader(test_dataset, batch_size=<span class="number">1</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluate the model</span></span><br><span class="line">model.<span class="built_in">eval</span>()  <span class="comment"># Set the model to evaluation mode</span></span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">        <span class="comment"># Ensure inputs have the correct shape</span></span><br><span class="line">        inputs, labels = inputs.<span class="built_in">float</span>(), labels</span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Accuracy of the model on the test images: <span class="subst">&#123; correct / total:<span class="number">.2</span>f&#125;</span>%&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Accuracy of the model on the test images: 0.90%</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>_NOTE_： 准确率还是可以的。</p>
<p><em>推理并且视觉验证</em></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Visualize some test samples and their predictions</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_predictions</span>(<span class="params">inputs, labels, predictions</span>):</span><br><span class="line">    fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">5</span>, figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        axes[i].imshow(inputs[i].squeeze(), cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">        axes[i].set_title(<span class="string">f&#x27;True: <span class="subst">&#123;labels[i]&#125;</span>, Pred: <span class="subst">&#123;predictions[i]&#125;</span>&#x27;</span>)</span><br><span class="line">        axes[i].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate test data</span></span><br><span class="line">visual_data, visual_labels = generate_dataset(num_samples=<span class="number">5</span>, img_size=<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the test dataset and data loader</span></span><br><span class="line">visual_dataset = ShapeDataset(visual_data, visual_labels, transform=transform)</span><br><span class="line">visual_loader = DataLoader(visual_dataset, batch_size=<span class="number">5</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.from_numpy(visual_data).float().unsqueeze(1).shape = 5,1,200,200</span></span><br><span class="line">outputs = model(torch.from_numpy(visual_data).<span class="built_in">float</span>().unsqueeze(<span class="number">1</span>))</span><br><span class="line">_, predictions = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">visualize_predictions(visual_data, visual_labels, predictions.numpy())</span><br></pre></td></tr></table></figure>
<p><img src="https://qiniu.oldzhangtech.com/mdpic/03db3b3d-0d90-4dac-9888-fcf3f9c809c2_39713376-9e7f-4693-93da-b60856f1a06f.png" alt="image.png"></p>
<h2 id="NOTE"><a href="#NOTE" class="headerlink" title="NOTE"></a>NOTE</h2><p><code>CNN</code> 是一种有效的 <code>解决空间</code> 信息的深度学习网络，特别适用于图像问题的解决中。</p>
]]></content>
      <categories>
        <category>学肥AI</category>
      </categories>
      <tags>
        <tag>学肥AI</tag>
        <tag>深度学习</tag>
        <tag>CNN</tag>
        <tag>LeNet</tag>
      </tags>
  </entry>
  <entry>
    <title>Serverless安装第三方package并构建layer</title>
    <url>/2024/12/09/%5B%E8%82%A5%E7%94%A8%E4%BA%91%E8%AE%A1%E7%AE%97%5D%20Serverless%E5%AE%89%E8%A3%85%E7%AC%AC%E4%B8%89%E6%96%B9package%E5%B9%B6%E6%9E%84%E5%BB%BAlayer/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>前面介绍的都没有进行额外的 package 安装的过程，所以本章就重点是 如何在 serverless 中安装第三方的 packag 和 如何去构建 layer 使得 package 可以复用。</p>
<blockquote>
<p>下面是后续所有例子需求：</p>
<ol>
<li>安装 emoji package，且运行</li>
</ol>
</blockquote>
<h2 id="安装方式：package-安装在本地"><a href="#安装方式：package-安装在本地" class="headerlink" title="安装方式：package 安装在本地"></a>安装方式：package 安装在本地</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">s init start-fc3-python -d demo1</span><br><span class="line">cd demo1</span><br><span class="line"><span class="meta prompt_"> </span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">安装依赖</span></span><br><span class="line">touch code/requirements.txt</span><br><span class="line">echo &quot;emoji==2.0.0&quot; &gt; code/requirements.txt</span><br><span class="line">pip3 install -r code/requirements.txt -t ./code</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>emoji package 就会安装在 local 的 code 文件夹中</p>
<p>修改 <code>code/index.py</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># @code/index.py</span></span><br><span class="line"><span class="keyword">from</span> emoji <span class="keyword">import</span> emojize</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">handler</span>(<span class="params">event, context</span>):</span><br><span class="line">    <span class="keyword">return</span> emojize(<span class="string">&quot;:thumbs_up:&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>部署代码</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">本地测试</span></span><br><span class="line">s local invoke</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">部署到远端</span></span><br><span class="line">s deploy</span><br></pre></td></tr></table></figure>



<p>当然部署也是可以上传代码的方式，如下图，其效果是一样的。</p>
<p><img src="https://qiniu.oldzhangtech.com/mdpic/7d8a2d84-885d-41b9-9176-1e453fbbf978_0a9076aa-14b3-429c-b353-b8a17a50d60c.png"></p>
<p><em><strong>结果：</strong></em></p>
<p><img src="https://qiniu.oldzhangtech.com/mdpic/bef8d5d1-00c5-41fe-9602-c53a43c0087b_83dd9e42-6950-4625-bc3c-20b18fef455d.png"></p>
<p><img src="https://qiniu.oldzhangtech.com/mdpic/d8ef8640-9b24-4ea9-9f98-018a77c1a333_662dc12b-8977-4705-97a5-ec7bcc63533f.png"></p>
<p>结果满足预期，但是他会把 emoji package 和 index.py 一起上传了。所以从代码大小中得知，整体会比较大。</p>
<h2 id="安装方式：构建-layer"><a href="#安装方式：构建-layer" class="headerlink" title="安装方式：构建 layer"></a>安装方式：构建 layer</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">s init start-fc3-python -d demo2</span><br><span class="line">cd demo2</span><br><span class="line"><span class="meta prompt_"> </span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">安装依赖</span></span><br><span class="line">touch code/requirements.txt</span><br><span class="line">echo &quot;emoji==2.0.0&quot; &gt; code/requirements.txt</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>修改 code&#x2F;index.py</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># @code/index.py</span></span><br><span class="line"><span class="keyword">from</span> emoji <span class="keyword">import</span> emojize</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">handler</span>(<span class="params">event, context</span>):</span><br><span class="line">    <span class="keyword">return</span> emojize(<span class="string">&quot;:thumbs_up:&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>关键代码来了，<strong>构建 layer，并且上传 layer。</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">s build --publish-layer</span><br><span class="line"></span><br><span class="line">[2024-12-09 18:00:12][INFO][hello_world] You need to add a new configuration env configuration dependency in yaml to take effect. The configuration is as follows:</span><br><span class="line">environmentVariables:</span><br><span class="line">  PYTHONPATH: /opt/python</span><br><span class="line">  </span><br><span class="line">layers:</span><br><span class="line">  - acs:fc:cn-shenzhen:1719759326012690:layers/demo2-layer/versions/1</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>构建成功后，提示说在 s.yaml 中添加上 environmentVariables 和 layers 节点。</p>
<p>💡 构建 layer 的意思就是把 requirements package 都构建一层 image layer 且上传到云端，后续的代码就是基于这层，不用另外的安装代码。如果有其他的项目想复用这些 package，直接改 s.yaml 的 layer 节点就可以了。</p>
<p>⚠️ 记得要把 requirements.txt 的文件要放在 和 index.py 的同级目录下。</p>
<p>所以往 s.yaml 里面写入上述的信息。</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">props:</span></span><br><span class="line">      <span class="string">...</span></span><br><span class="line">      <span class="attr">environmentVariables:</span></span><br><span class="line">        <span class="attr">PYTHONPATH:</span> <span class="string">/opt/python</span></span><br><span class="line">      <span class="attr">layers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">acs:fc:cn-shenzhen:1719759326012690:layers/demo2-layer/versions/1</span></span><br></pre></td></tr></table></figure>



<p>部署代码</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">本地测试</span></span><br><span class="line">s local invoke</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">部署到远端</span></span><br><span class="line">s deploy</span><br></pre></td></tr></table></figure>

<p><em>****</em></p>
<p><em><strong>结果：</strong></em></p>
<p><img src="https://qiniu.oldzhangtech.com/mdpic/3182e155-8a24-4d6e-80bf-72c8890ebdf5_e3de1325-4c7c-4b4f-8ce5-cc6c9d2bb8a4.png"></p>
<p>因为不会上传 emoji package，代码明显是小了很多。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol>
<li>serverless 就是 业务 和 trigger 隔离开来，本文章例子中都是完成 业务，没有 trigger；只要后续补充上 trigger，这个业务就可以串通了。</li>
<li>部署方式，优先是构建 layer，后上传代码，因为 layer 可以复用，且减少项目的代码。</li>
<li>如果开发&#x2F;本地测试&#x2F;部署，可以遵循下面的方法：</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">初始化</span></span><br><span class="line">s init start-fc3-python -d demo2</span><br><span class="line">cd demo2</span><br><span class="line"><span class="meta prompt_"> </span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">安装依赖</span></span><br><span class="line">touch code/requirements.txt</span><br><span class="line">echo &quot;emoji==2.0.0&quot; &gt; code/requirements.txt</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">本地测试</span></span><br><span class="line">s local invoke</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">构建 layer</span></span><br><span class="line">s build --publish-layer</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">部署</span></span><br><span class="line">s deploy</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="资料"><a href="#资料" class="headerlink" title="资料"></a>资料</h2><ol>
<li><a href="https://gitee.com/oldmanzhang/practice_serverlsess/tree/master/p03">源代码</a></li>
<li><a href="https://help.aliyun.com/zh/functioncompute/fc-3-0/user-guide/request-handlers?spm=a2c4g.11186623.4.2.72757bbcKfadgi&scm=20140722.H_2512964._.ID_2512964-OR_rec-V_1">阿里云 serverless 说明</a></li>
<li><a href="https://manual.serverless-devs.com/user-guide/aliyun/fc3/build/">Serverless Devs Docs</a></li>
<li><a href="https://github.com/devsapp/start-fc?spm=5176.fcnext.0.0.18e978c8K4qMDn">start-fc-template</a></li>
</ol>
]]></content>
      <categories>
        <category>肥用云计算</category>
      </categories>
      <tags>
        <tag>serverless</tag>
      </tags>
  </entry>
  <entry>
    <title>RNN 的基础</title>
    <url>/2024/08/06/%5B%E5%AD%A6%E8%82%A5AI%5D%20RNN%20%E7%9A%84%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<h2 id="背景问题"><a href="#背景问题" class="headerlink" title="背景问题"></a>背景问题</h2><p>我们有时候需要处理一些 _时间序列数据_，比如 股票预测，天气预测，根据前面单词预测后续一个单词等场景。NN（Neural Network） 在处理这些问题，有他的局限性。RNN （Recurrent Neural Network），就是为了用于处理这些问题。</p>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>一句话：能够记住前 X 步，离当前时间越近的，越记得深刻，即越能影响当前步的输出。</p>
<p>里面有几层意思：</p>
<ol>
<li>当前步输出，与前 X 步的记忆构成</li>
<li>前 X 步的输入，就构成了 前 X 步的记忆</li>
<li>前 X 步的记忆，X 越小，权重越大，越能影响当前步的输出</li>
</ol>
<p>和人的记忆就很像，越近的事件，当然记得越清了，越往前倒，就记得越模糊。</p>
<p>简单的示意图，颜色越红，就是记得越清楚的。越往后，<code>“what”</code> 这个单词就已经不记得了。<br><img src="https://qiniu.oldzhangtech.com/mdpic/d972ff24-a2e9-4c69-b9f2-c13898cbba5d_f74254d1-fcc7-4ab7-a678-a74fa78fd7ee.png"></p>
<p>💡 所以搞清清楚 <strong>记忆</strong>，<strong>输入</strong>，<strong>输出</strong>，<strong>时间步</strong> 之间的关系，是 RNN 的关键。（NN 就没有 <strong>记忆</strong> 和 <strong>时间步</strong> 的概念）</p>
<p>时间步，就是表示时间序列。比如 股市指标的每秒钟时刻，天气浮动的每天时刻，一句话中的每个单词的前后顺序时刻，他们都是有先后关系的。<br>记忆，就好像 **每个时间步 **的一个内存，他记录着当前步的信息。我们可以使用更加专业的 <strong>隐状态（Hidden State）</strong>来表示。他随着时间步的递进，里面保存的消息慢慢弱化，直至消失。</p>
<h3 id="原理图"><a href="#原理图" class="headerlink" title="原理图"></a>原理图</h3><h3 id=""><a href="#" class="headerlink" title=""></a><img src="https://qiniu.oldzhangtech.com/mdpic/c332685c-2b24-4bba-a779-05fd81b85263_01dd4e9d-5ea6-446b-b12b-b23e191501f3.png"></h3><p>上述是 RNN 的原理图。<br>h: 是隐状态，x: 是输入，o: 是输出， 上面 3 个变量都是与时间步相关的，有时间标注。<br>图中的意思是： t 时刻的 h（隐状态），由 t-1 时刻的 h（隐状态）和 t 时刻的 x（输入）生成；t 时刻的 o（输出），由 t 时刻的 h（隐状态）生成。</p>
<p>总结成下面的两个公式:<br>$h_t &#x3D; f(x_t, h_{t-1})$<br>$o_t &#x3D; g(h_t)$   </p>
<p>因为  又可以展开来 $[ h_{t-1} &#x3D; f(x_{t-1}, h_{t-2})]$  ，且 $h_{t-2}$ 也同样展开，这样延续下去，就可以无限展开。<br>因为硬件限制和计算限制，我们会有 时间步 sequence 的概念，就是当前步的预测，仅仅是与前 n 步的隐状态相关，n 就是 sequence 的意思。<br><img src="https://qiniu.oldzhangtech.com/mdpic/7e01c973-ba9d-4ab3-bda4-0132910d5ff5_fedcb4a7-55ad-4e45-9454-c26bda653585.png" alt="image.png"><br>比如上图， sequence &#x3D; 4， 长方形就是 隐状态，长度就是权重，越往左边，代表时间越久远，长度越矮，隐状态的权重越低。后面就会逐渐消失，所以 $h_{t-5}$  对 $h_{t}$  就不会再起作用了。</p>
<p>整体的公式非常的简单，可以记住，或者不用记忆，了解大致通俗的意思也是可以的。下面手写 RNN 的组件就把上面的公式 用代码展现出来。</p>
<h2 id="手写-RNN-组件"><a href="#手写-RNN-组件" class="headerlink" title="手写 RNN 组件"></a>手写 RNN 组件</h2><p>根据上述的说明，形成 <strong>隐状态</strong>，是整个 <code>RNN</code> 的关键。手动实现 <code>RNN</code> 组件，就能了解他的内部运行机制是什么，利于理解他的原理。</p>
<blockquote>
<p>手写组件 是为了对 <code>RNN</code> 理解更加深入，后续都使用 <code>nn.RNN</code> 已经封装好的方法，不再使用手写 RNN。可运行 ipynb 文件<a href="https://gitee.com/oldmanzhang/resource_machine_learning/blob/master/deep_learning/rnn_basic.ipynb">链接</a>。</p>
</blockquote>
<p>下面是模拟有 10 个时刻（ sequence 变量 ）的输入数据，从 <em>输入</em> 形成 _隐状态_，并且从 <em>隐状态</em> 生成 <em>输出</em> 的过程。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hidden_size = <span class="number">20</span> <span class="comment"># 记忆体的维度</span></span><br><span class="line">input_size = <span class="number">1</span> <span class="comment"># 输入特征</span></span><br><span class="line">output_size = <span class="number">1</span> <span class="comment"># 输出特征</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入的映射关系，更新 记忆体的数据</span></span><br><span class="line">i2h = nn.Linear(input_size + hidden_size, hidden_size)  <span class="comment"># 输入到隐藏层</span></span><br><span class="line"><span class="comment"># 从 记忆体的数据 生成 输出数据</span></span><br><span class="line">i2o = nn.Linear(hidden_size, output_size)  <span class="comment"># 隐藏层到输出层</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化一个输入，格式是就是 (sequence, input_feature)</span></span><br><span class="line">sequence = <span class="number">10</span></span><br><span class="line">X = torch.rand(sequence,<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 初始化一个记忆体, sequence, feature</span></span><br><span class="line">hidden = torch.zeros(<span class="built_in">len</span>(X), hidden_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># [1] 混合 输入 和 前一个记忆体</span></span><br><span class="line">combined = torch.cat((X, hidden), <span class="number">1</span>)  <span class="comment"># torch.Size([1, 21])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># [2] 形成 新的记忆体</span></span><br><span class="line">hidden = i2h(combined)</span><br><span class="line"><span class="comment"># [3] 输出 output</span></span><br><span class="line">output = i2o(hidden)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 后续加上 损失函数 和 优化函数 和 反向传播，就是一个完整的 RNN 的流程。</span></span><br><span class="line"> </span><br></pre></td></tr></table></figure>
<p>上述就是一个最简单的 RNN 组件的实现逻辑，代码简单清晰，就不再一句句说明。</p>
<p>有几个需要注意的地方：</p>
<ol>
<li>[1] + [2] 即是  $f(x_t , h_{t-1})$ ，  <code>torch.cat + i2h (full connect)</code> ，输出 $h_t$<code>当前 hidden_state</code> 。</li>
<li>[3] 即是 $g(h_t)$ ， <code>i2o (hidden)</code>，输出 $o_t$当前输出，就是从 <code>当前hidden_state</code> 中来</li>
<li>上述代码整合到一个 model 里面，循环起来，就会是一个手写 RNN。</li>
<li>[1] 中，<code>torch.cat((X, hidden), 1)</code> 是 <code>10 * input</code> 和 <code>hidden_state</code> 混合建立联系；当然可以 $f(x_1, hidden_0)$; $f(x_2, hidden_1)$; $f(x_3, hidden_2)$ … $f(x_10, hidden_9)$ 逐步堆叠 <code>hidden_state</code>，这样会更加的直观。</li>
</ol>
<h2 id="例子-预测字符"><a href="#例子-预测字符" class="headerlink" title="例子 - 预测字符"></a>例子 - 预测字符</h2><p>目的：由“2 个字符”，预测下一个字符，同时连续猜测下去。<br>训练数据： “hello world”。</p>
<p><strong><em>分析过程：</em></strong></p>
<ol>
<li>是 <strong>分类</strong>问题</li>
<li>“2 个字符” 是一个有连续先后关系的输入，所以使用 RNN</li>
<li>定义 RNN 的时候 input_feature 和 output_feature 都是 8（字符串的所有唯一值总数是 8）</li>
<li>X 的 sequence 可以定义成 2</li>
</ol>
<p>详细的代码可以直接访问 ipynb 文件<a href="https://gitee.com/oldmanzhang/resource_machine_learning/blob/master/deep_learning/rnn_classifications_charPredit.ipynb">链接</a>。<br>训练后的结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 测试模型</span></span><br><span class="line">start_str = <span class="string">&quot;he&quot;</span></span><br><span class="line">predict_times = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###### output predict</span></span><br><span class="line"><span class="comment"># Predicted string: hello w</span></span><br></pre></td></tr></table></figure>

<h2 id="例子-预测-GS10"><a href="#例子-预测-GS10" class="headerlink" title="例子 - 预测 GS10"></a>例子 - 预测 GS10</h2><p>目的：由前 6 年数据，是否可以预测到 下一年的数据<br>训练数据： 美国国债收益率数据<br><strong><em>分析过程：</em></strong></p>
<ol>
<li>是 <strong>回归</strong>问题</li>
<li>“前 6 年数据” 预测下一年，就是有一个先后时间关系，所以使用 RNN</li>
<li>定义 RNN 的时候 input_feature 和 output_feature 都是 1（因为都是 数值上是 1 个值的回归）</li>
<li>X 的 sequence 可以定义成 6</li>
</ol>
<p>详细的代码可以直接访问 ipynb 文件<a href="https://gitee.com/oldmanzhang/resource_machine_learning/blob/master/deep_learning/rnn_regresssion_gs10.ipynb">链接</a>。<br>训练后的结果：<br><img src="https://qiniu.oldzhangtech.com/mdpic/3cf5a41c-6742-4031-afe8-d7931e87970f_48c9e63a-60a4-4845-8333-f73de20a0504.png" alt="image.png"></p>
<h2 id="代码注意问题"><a href="#代码注意问题" class="headerlink" title="代码注意问题"></a>代码注意问题</h2><blockquote>
<p>集中在调用 nn.RNN 的问题</p>
</blockquote>
<h3 id="nn-RNN-的-input-feature-和-X-维度关系"><a href="#nn-RNN-的-input-feature-和-X-维度关系" class="headerlink" title="nn.RNN 的 input_feature 和 X 维度关系"></a>nn.RNN 的 input_feature 和 X 维度关系</h3><p>在整个训练过程中，调整 <code>model(X)</code> 中的 <code>X</code> 维度，是最耗费时间的。如果语意上去理解了，那么用起来就会得心应手很多。</p>
<p>🔥 注意点</p>
<ol>
<li>比如：一个 X.shape &#x3D; (5,3,10)，就可以理解成  <strong>5 个样本, 3 个时间步, 每个时间步有 10 个特征</strong></li>
<li><code>X dim</code> <strong>至少是 2 维</strong>，分别是 <code>(sequence, feature)</code>; 如果是 3 维，分别是 <code>(batch, sequence, feature)</code>；当然 也可以大于 2 维。</li>
</ol>
<p>可运行文件<a href="https://gitee.com/oldmanzhang/resource_machine_learning/blob/master/deep_learning/rnn_basic.ipynb">链接</a></p>
<p><strong>分类问题</strong>, X.dim &#x3D; 3</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 RNN 层</span></span><br><span class="line"><span class="comment"># input_size 理解成 特征会比较好</span></span><br><span class="line">rnn = nn.RNN(input_size=<span class="number">10</span>, hidden_size=<span class="number">20</span>, num_layers=<span class="number">1</span>, batch_first=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建输入张量 (batch_size, seq_length, input_size)</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">5</span>, <span class="number">3</span>, <span class="number">10</span>)  <span class="comment"># 5 个样本, 3 个时间步, 每个时间步有 10 个特征</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化隐藏状态 (num_layers, batch_size, hidden_size)</span></span><br><span class="line">h0 = torch.zeros(<span class="number">1</span>, <span class="number">5</span>, <span class="number">20</span>)  <span class="comment"># 1 层, 5 个样本, 每层隐藏状态有 20 个特征</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行前向传递</span></span><br><span class="line">output, hn = rnn(input_tensor, h0)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印输入和输出的形状</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Input shape:&quot;</span>, input_tensor.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output shape:&quot;</span>, output.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Hidden state shape:&quot;</span>, hn.shape)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Input shape: torch.Size([5, 3, 10])</span></span><br><span class="line"><span class="string">Output shape: torch.Size([5, 3, 20])</span></span><br><span class="line"><span class="string">Hidden state shape: torch.Size([1, 5, 20])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>同样是 rnn 的 model, X.dim &#x3D; 2，一样可以运行，那么他就代表了 (seq_length, input_size)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建输入张量 (seq_length, input_size)</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">3</span>, <span class="number">10</span>)  <span class="comment"># 3 个时间步, 每个时间步有 10 个特征</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化隐藏状态 (num_layers, batch_size, hidden_size)</span></span><br><span class="line">h0 = torch.zeros(<span class="number">1</span>, <span class="number">20</span>)  <span class="comment"># 1 层, 5 个样本, 每层隐藏状态有 20 个特征</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行前向传递</span></span><br><span class="line">output, hn = rnn(input_tensor, h0)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Input shape:&quot;</span>, input_tensor.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output shape:&quot;</span>, output.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Hidden state shape:&quot;</span>, hn.shape)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Input shape: torch.Size([3, 10])</span></span><br><span class="line"><span class="string">Output shape: torch.Size([3, 20])</span></span><br><span class="line"><span class="string">Hidden state shape: torch.Size([1, 20])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>回归问题</strong>，X.dim &#x3D; 3</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 回归的时候，基本上 1 个特征输入，1 个特征输出</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 RNN 层</span></span><br><span class="line"><span class="comment"># input_size 理解成 特征会比较好</span></span><br><span class="line">rnn = nn.RNN(input_size=<span class="number">1</span>, hidden_size=<span class="number">20</span>, num_layers=<span class="number">1</span>, batch_first=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建输入张量 (batch_size, seq_length, input_size)</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">5</span>, <span class="number">3</span>, <span class="number">1</span>)  <span class="comment"># 5 个样本, 3 个时间步, 每个时间步有 1 个特征</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化隐藏状态 (num_layers, batch_size, hidden_size)</span></span><br><span class="line">h0 = torch.zeros(<span class="number">1</span>, <span class="number">5</span>, <span class="number">20</span>)  <span class="comment"># 1 层, 5 个样本, 每层隐藏状态有 20 个特征</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行前向传递</span></span><br><span class="line">output, hn = rnn(input_tensor, h0)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印输入和输出的形状</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Input shape:&quot;</span>, input_tensor.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output shape:&quot;</span>, output.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Hidden state shape:&quot;</span>, hn.shape)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Input shape: torch.Size([5, 3, 1])</span></span><br><span class="line"><span class="string">Output shape: torch.Size([5, 3, 20])</span></span><br><span class="line"><span class="string">Hidden state shape: torch.Size([1, 5, 20])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>这里的 output 需要经过一个全连接层，才最后转换成真正的 output。比如下面的代码，利用上面的回归例子，模拟 output_size &#x3D;1 进行输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#因为 RNN 层是没有 output_size,手动添加一个用于 output_size，比如回归输出 1 个特征</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 20个特征里面，提取一个输出</span></span><br><span class="line">out_linear = nn.Linear(in_features=<span class="number">20</span>, out_features=<span class="number">1</span>)</span><br><span class="line">output2 = out_linear(output)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output shape:&quot;</span>, output2.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 与 input 对应起来了，就是 # 5 个样本, 3 个时间步, 每个时间步有 1 个特征</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Output shape: torch.Size([5, 3, 1])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="nn-RNN-的-target-与-y-的数据的处理"><a href="#nn-RNN-的-target-与-y-的数据的处理" class="headerlink" title="nn.RNN 的  target 与 y 的数据的处理"></a>nn.RNN 的  target 与 y 的数据的处理</h3><p>还是上述的例子，回归问题</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 回归的时候，基本上 1 个特征输入，1 个特征输出</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 RNN 层</span></span><br><span class="line"><span class="comment"># input_size 理解成 特征会比较好</span></span><br><span class="line">rnn = nn.RNN(input_size=<span class="number">1</span>, hidden_size=<span class="number">20</span>, num_layers=<span class="number">1</span>, batch_first=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建输入张量 (batch_size, seq_length, input_size)</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">5</span>, <span class="number">3</span>, <span class="number">1</span>)  <span class="comment"># 5 个样本, 3 个时间步, 每个时间步有 1 个特征</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化隐藏状态 (num_layers, batch_size, hidden_size)</span></span><br><span class="line">h0 = torch.zeros(<span class="number">1</span>, <span class="number">5</span>, <span class="number">20</span>)  <span class="comment"># 1 层, 5 个样本, 每层隐藏状态有 20 个特征</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行前向传递</span></span><br><span class="line">output, hn = rnn(input_tensor, h0)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印输入和输出的形状</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Input shape:&quot;</span>, input_tensor.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output shape:&quot;</span>, output.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Hidden state shape:&quot;</span>, hn.shape)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Input shape: torch.Size([5, 3, 1])</span></span><br><span class="line"><span class="string">Output shape: torch.Size([5, 3, 20])</span></span><br><span class="line"><span class="string">Hidden state shape: torch.Size([1, 5, 20])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#因为 RNN 层是没有 output_size,手动添加一个用于 output_size，比如回归输出 1 个特征</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 20个特征里面，提取一个输出</span></span><br><span class="line">out_linear = nn.Linear(in_features=<span class="number">20</span>, out_features=<span class="number">1</span>)</span><br><span class="line">output2 = out_linear(output)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output shape:&quot;</span>, output2.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 与 input 对应起来了，就是 # 5 个样本, 3 个时间步, 每个时间步有 1 个特征</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Output shape: torch.Size([5, 3, 1])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>因为训练数据 X.shape &#x3D; torch.Size([5, 3, 1]) <code>5 个样本, 3 个时间步, 每个时间步有 1 个特征</code>，得到 output 都是 shape 与 X 一致，都是 torch.Size([5, 3, 1])。 但是我们仅仅是需要 <strong>最后时间步</strong> 的数据，才是我们想要的数据，即 output[:, -1, :] 就是我们想要的数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#因为 RNN 层是没有 output_size,手动添加一个用于 output_size，比如回归输出 1 个特征</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 20个特征里面，提取一个输出</span></span><br><span class="line">out_linear = nn.Linear(in_features=<span class="number">20</span>, out_features=<span class="number">1</span>)</span><br><span class="line">output2 = out_linear(output[:,-<span class="number">1</span>,:])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output shape:&quot;</span>, output2.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 与 input 对应起来了，就是 # 5 个样本, 3 个时间步, 每个时间步有 1 个特征</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Output shape: torch.Size([5, 1])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p><code>output[:, -1, :]</code> 取 最后时间步 的数据。这是一种取巧的方法。</p>
<p>但是有时候，他并不是输出最后时间步的数据，所以记得留意构建 target（真实值） 的时候，注意是否从 model 出来的 y（预测值） 的维度之间的差异。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="隐状态"><a href="#隐状态" class="headerlink" title="隐状态"></a>隐状态</h3><p>他是一个中间状态，介于输入和输出之间，或者说记忆力，他是 RNN 的一个关键概念。<br>他在后续的 decode&amp;encode 的模型中，都发挥着重要的作用。</p>
<h3 id="优点和场景"><a href="#优点和场景" class="headerlink" title="优点和场景"></a>优点和场景</h3><p>序列数据的任务适合使用 RNN。</p>
<ol>
<li>股票预测</li>
<li>自然语言处理</li>
</ol>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol>
<li>时间步过长，会导致前面的隐状态会给忘记了</li>
<li>梯度消失和梯度爆炸</li>
<li>计算效率相对比较低</li>
</ol>
<h2 id="-1"><a href="#-1" class="headerlink" title=""></a></h2>]]></content>
      <categories>
        <category>学肥AI</category>
      </categories>
      <tags>
        <tag>学肥AI</tag>
        <tag>深度学习</tag>
        <tag>CNN</tag>
        <tag>NN</tag>
        <tag>卷积神经网络</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
</search>
